import fs from 'fs';

const data = JSON.parse(fs.readFileSync('public/data/initial-papers.json', 'utf-8'));

const eqUpdates = {
  6: [
    { name: "비국소 블록", desc: "특정 위치 i의 값을 계산할 때, 가까운 이웃뿐 아니라 모든 위치 j의 정보를 가중합한다. 가중치 f(x_i, x_j)는 두 위치가 얼마나 비슷한지를 나타내며, C(x)는 정규화 상수이다." },
    { name: "임베디드 가우시안 유사도", desc: "두 위치 간 유사도를 측정하는 함수로, 각 위치를 학습된 임베딩으로 변환한 뒤 내적의 소프트맥스를 취한다. 값이 클수록 두 위치가 강하게 관련되어 있다는 의미이다." }
  ],
  7: [
    { name: "인코더 복잡도", desc: "UE 측 인코더의 총 연산량(FLOPs)을 각 합성곱 레이어별 연산량의 합으로 나타낸다. 이 값을 최소화하는 것이 경량 인코더 설계의 핵심 목표이다." },
    { name: "비대칭 설계 비율", desc: "인코더 복잡도를 디코더 복잡도로 나눈 비율이다. 1보다 훨씬 작다는 것은 인코더(UE 측)가 디코더(BS 측)보다 훨씬 가볍게 설계되었다는 뜻이다." }
  ],
  8: [
    { name: "균일 양자화", desc: "연속 실수 값을 일정 간격(Δ)으로 나눈 격자점에 반올림하는 연산이다. B비트이면 2^B개의 레벨로 나누며, Δ는 값 범위를 레벨 수로 나눈 것이다." },
    { name: "직통 추정기 (STE)", desc: "양자화는 계단 함수라 미분이 0이 되어 학습이 불가능한 문제가 있다. STE는 역전파 시 양자화 단계를 항등 함수로 대체하여, 그래디언트가 그대로 통과하게 하는 우회 기법이다." },
    { name: "총 피드백 비트 수", desc: "실제로 전송해야 하는 총 비트 수 = 코드워드 길이(M) × 원소당 비트 수(B)이다. 이 값이 곧 무선 채널을 차지하는 피드백 오버헤드가 된다." }
  ],
  9: [
    { name: "차분 CSI", desc: "현재 CSI에서 이전에 복원한 CSI를 빼서 \"변한 부분만\" 추출한 것이다. CSI가 천천히 변하면 ΔH가 매우 작아져 적은 비트로도 전달 가능하다." },
    { name: "시간적 피드백 감소", desc: "차분 CSI(ΔH)를 복원한 결과의 NMSE이다. 전체 CSI 대신 차분만 인코딩하므로 동일 비트에서 더 낮은 NMSE를 달성할 수 있다." }
  ],
  10: [
    { name: "가중치 이진화", desc: "가중치를 +1 또는 -1로 이진화하되, L1 노름 기반의 스케일링 팩터(α)를 곱해 크기 정보를 보존한다. αb ≈ W로, 이진 가중치 b와 실수 스케일 α의 곱으로 원본 가중치를 근사한다." },
    { name: "XNOR-비트카운트 연산", desc: "이진화된 입력과 가중치의 내적을 비트 연산으로 계산하는 방법이다. 곱셈이 XNOR로, 덧셈이 비트카운트(popcount)로 대체되어 일반 행렬곱 대비 수십 배 빠르다." }
  ],
  11: [
    { name: "지식 증류 손실", desc: "학생 네트워크의 손실 = ①CSI 복원 오차(MSE) + ②교사의 중간 특징맵과 학생의 중간 특징맵 차이. ②가 \"교사의 노하우를 모방\"하는 항으로, 학생이 교사처럼 특징을 추출하도록 유도한다." },
    { name: "학생 네트워크 파라미터 감소", desc: "교사 대비 학생이 얼마나 작은지를 나타내는 비율이다. 예를 들어 0.8이면 교사의 80% 파라미터를 제거했다는 뜻이며, 그래도 증류 덕분에 성능 저하가 적다." }
  ],
  12: [
    { name: "공간 주의", desc: "CSI 행렬의 \"어떤 공간 위치가 중요한가\"를 학습하는 맵이다. 채널 축으로 평균 풀링과 최대 풀링을 한 결과를 합성곱으로 결합하여, 위치별 0~1 가중치를 생성한다." },
    { name: "채널 주의", desc: "\"어떤 특징 채널이 유용한가\"를 학습하는 가중치이다. 전역 평균 풀링과 최대 풀링으로 채널별 요약을 만들고, 공유 MLP를 통과시켜 채널별 0~1 가중치를 출력한다." }
  ],
  13: [
    { name: "자기회귀 CSI 예측", desc: "CSI 행렬의 원소를 하나씩 순서대로 예측하는 방식이다. 각 원소는 \"지금까지 본 원소들\"에 조건부로 생성되며, GPT가 다음 토큰을 예측하는 것과 같은 원리이다." },
    { name: "연합 집계", desc: "여러 기지국(로컬 모델)의 학습 결과를 중앙에서 평균내어 글로벌 모델을 업데이트한다. 각 기지국의 데이터 양에 비례하여 가중 평균을 취하며, 원본 데이터를 공유하지 않아 프라이버시가 보호된다." }
  ],
  14: [
    { name: "점진적 인코딩", desc: "하나의 인코더가 점점 더 긴 코드워드를 출력한다. 처음 M₁개 원소만으로 대략적 복원이 가능하고, 추가 원소가 디테일을 보완한다. 마치 저화질 → 고화질로 점진 로딩하는 것과 같다." },
    { name: "적응적 레이트 선택", desc: "목표 품질(ε)을 만족하는 최소 코드워드 길이 M_k를 선택한다. 채널이 단순하면 짧은 코드워드로 충분하고, 복잡하면 긴 코드워드를 사용하여 통신 자원을 효율적으로 쓴다." }
  ],
  15: [
    { name: "혼합 정밀도 목적 함수", desc: "전체 비트 예산(B_total) 제약 하에서, 각 레이어의 비트폭(b_l)을 선택하여 최종 NMSE를 최소화하는 최적화 문제이다. 모든 레이어를 같은 비트로 양자화하는 것보다 효율적이다." },
    { name: "레이어 민감도", desc: "한 레이어의 비트폭을 8비트에서 4비트로 줄였을 때 NMSE가 얼마나 나빠지는지를 측정한 값이다. 민감도가 높은 레이어는 비트를 많이 유지해야 한다." }
  ],
  16: [
    { name: "필터 중요도 점수", desc: "합성곱 필터의 가중치 절대값 합(L1 norm)으로 중요도를 매긴다. 값이 큰 필터는 출력에 큰 영향을 미치므로 보존하고, 작은 필터는 제거해도 성능 영향이 적다." },
    { name: "가지치기 비율", desc: "전체 필터 중 제거된 필터의 비율이다. 예를 들어 0.5이면 절반의 필터를 제거한 것이며, 비율이 높을수록 모델이 가벼워지지만 성능 저하 위험이 커진다." }
  ],
  17: [
    { name: "레이트-왜곡 최적화", desc: "복원 품질(왜곡, NMSE)과 전송 비용(레이트, 비트 수)을 동시에 최적화한다. 라그랑주 승수 λ가 둘 사이의 균형을 조절하며, λ가 크면 비트를 아끼는 쪽으로, 작으면 품질을 높이는 쪽으로 학습된다." },
    { name: "적응적 비트 선택", desc: "보조 네트워크 g가 코드워드 s의 특성을 보고, 각 원소에 몇 비트를 할당할지 자동으로 결정한다. 정보가 많은 원소에는 비트를 많이, 적은 원소에는 비트를 적게 배정한다." }
  ],
  18: [
    { name: "채널 셔플", desc: "그룹 합성곱 후 그룹 간 정보를 섞어주는 연산이다. 특징맵을 (그룹 수 × 채널/그룹)으로 재구성한 뒤 전치하고 다시 펴서, 각 그룹이 다른 그룹의 정보를 받을 수 있게 한다." },
    { name: "그룹 합성곱 FLOPs", desc: "표준 합성곱의 FLOPs를 그룹 수(g)로 나눈 것이 그룹 합성곱의 FLOPs이다. g=4이면 연산량이 1/4로 줄어들며, 채널 셔플과 함께 사용하면 정보 교류도 유지된다." }
  ],
  19: [
    { name: "벡터 양자화", desc: "인코더 출력 벡터 s를 코드북의 모든 벡터와 거리(유클리드)를 비교하여, 가장 가까운 벡터 e_k로 교체한다. 전송 시에는 인덱스 k만 보내면 되므로 비트 효율이 높다." },
    { name: "VQ-VAE 손실 함수", desc: "세 가지 손실의 합이다. ①복원 손실: CSI를 잘 복원하도록. ②코드북 손실: 코드북 벡터가 인코더 출력에 가까워지도록. ③커미트먼트 손실: 인코더 출력이 코드북에 가까워지도록. sg는 그래디언트 차단 연산자이다." }
  ],
  20: [
    { name: "확산 순방향 과정", desc: "원본 CSI에 스케줄(β_t)에 따라 가우시안 노이즈를 점진적으로 추가하여, 최종적으로 순수 노이즈가 되게 하는 과정이다. T 단계 후에는 원본 정보가 거의 사라진다." },
    { name: "조건부 역방향 과정", desc: "순수 노이즈에서 시작하여, 수신된 코드워드 s를 힌트로 삼아 한 단계씩 노이즈를 제거해 나가며 CSI를 복원한다. 각 단계에서 학습된 신경망이 제거할 노이즈를 예측한다." }
  ],
  21: [
    { name: "결합 최적화", desc: "인코더(f_e)로 압축 → 양자화기(Q)로 이산화 → 디코더(f_d)로 복원하는 전체 파이프라인을 하나의 손실 함수로 동시 최적화한다. λR은 전송률 페널티로, 비트 수를 줄이는 방향으로 유도한다." },
    { name: "레이트 추정", desc: "양자화된 코드워드의 실제 전송 비트 수를 엔트로피 모델로 추정한다. 엔트로피가 높은(패턴이 불규칙한) 코드워드는 더 많은 비트가 필요하며, 이를 손실에 반영하여 비트를 줄이도록 학습한다." }
  ],
  22: [
    { name: "삼진 양자화", desc: "가중치를 {-α, 0, +α}로 양자화하는 연산이다. 절대값이 임계값 Δ보다 크면 부호에 따라 +α/-α로, 작으면 0으로 설정한다. 0은 해당 연결을 무시한다는 의미이다." },
    { name: "최적 임계값", desc: "임계값 Δ를 가중치 절대값 평균의 약 0.7배로 설정하면 양자화 오차가 최소화된다는 경험적 결과이다. 이 간단한 공식으로 별도 탐색 없이 좋은 임계값을 얻는다." }
  ],
  23: [
    { name: "지역 인식 어텐션", desc: "전역 셀프 어텐션 대신, 각 위치의 주변 영역(지역 마스크 M)에만 어텐션을 적용한다. 전역 어텐션보다 계산이 가볍고, CSI 행렬에서 인접한 부반송파/안테나 간 상관관계에 집중한다." }
  ],
  24: [
    { name: "SwinLSTM 셀", desc: "Swin Transformer의 이동 창(Shifted Window) 어텐션으로 공간적 관계를 포착하고, LSTM으로 시간적 변화를 추적하는 결합 셀이다. 한 블록에서 공간·시간 정보를 동시에 처리한다." }
  ],
  25: [
    { name: "적응적 결합 손실", desc: "학습 초반에는 복원(NMSE)에, 후반에는 양자화 왜곡에 더 집중하도록 가중치 α를 적응적으로 조절하는 손실 함수이다. 로그 스케일을 사용하여 큰 오차에 민감하게 반응한다." },
    { name: "비균일 비트 할당", desc: "코드워드 각 원소의 분산(값의 변동 폭)에 기반하여 비트를 배분한다. 분산이 큰 원소는 표현 범위가 넓어야 하므로 비트를 많이, 분산이 작은 원소는 비트를 적게 할당한다." }
  ],
  26: [
    { name: "가역 변환", desc: "아핀 커플링 레이어로, 입력을 두 그룹으로 나눠 한쪽이 다른 쪽의 변환 파라미터를 생성한다. 역변환이 정확히 존재하므로, 같은 네트워크를 뒤집어 돌리면 완벽한 복원이 가능하다(정보 손실 없음)." },
    { name: "DAQ 양자화기", desc: "각 코드워드 원소마다 양자화 스텝 크기(Δ_i)를 학습하는 미분 가능 양자화기이다. 원소별로 최적 정밀도를 자동 조정하며, STE를 통해 역전파가 가능하다." }
  ],
  27: [
    { name: "의미적 CSI 임베딩", desc: "CSI 행렬을 패치로 나누고, CQI(Channel Quality Indicator) 등의 의미적 임베딩을 함께 연결하여 Transformer 인코더에 입력한다. 통신 품질에 중요한 특징을 우선적으로 학습한다." }
  ],
  28: [
    { name: "잔차 확산", desc: "1단계 오토인코더 복원의 오차(잔차)를 확산 모델이 예측하여 제거한다. 원본과 초기 복원의 차이를 추가로 보정하는 구조로, 확산 단계 수를 조절하면 품질-속도 트레이드오프를 조정할 수 있다." }
  ],
  29: [
    { name: "S-R 전문가 혼합 레이어", desc: "공유 전문가(W_s)가 항상 활성화되어 공통 지식을 담당하고, 게이팅 네트워크가 입력에 따라 상위 K개의 라우팅 전문가를 추가로 선택한다. 환경별로 다른 전문가가 활성화되어 적응적 처리가 가능하다." }
  ],
  30: [
    { name: "채널 클러스터 분해", desc: "CSI를 SVD로 분해하여 K개의 채널 클러스터로 나누고, 학습 환경(소스)과 새 환경(타겟)의 클러스터 분포를 정렬한다. 분포 차이를 줄여 새로운 환경에서도 성능이 유지되게 한다." }
  ],
  31: [
    { name: "비전-CSI 코드북", desc: "대규모 비전 모델의 풍부한 특징 공간에서 CSI 양자화에 최적화된 코드북을 구성한다. 비전 모델이 학습한 범용 표현을 활용하여, 적은 CSI 데이터로도 효과적인 코드북을 얻는다." }
  ],
  32: [
    { name: "의미적 파일럿 신뢰도", desc: "LLM이 복호한 심볼에 대한 신뢰도(확률)가 임계값 τ를 넘으면 해당 심볼을 시맨틱 파일럿으로 사용한다. LLM의 언어 이해력을 활용하여, 채널 추정에 유용한 심볼을 자동 선별한다." }
  ],
  33: [
    { name: "VQ-VAE 손실 함수", desc: "세 항의 합으로 구성된다. ①복원 손실: 입력을 잘 재현하도록. ②코드북 손실: 코드북 벡터를 인코더 출력 쪽으로 이동. ③커미트먼트 손실: 인코더 출력을 코드북 쪽으로 이동. sg(stop-gradient)가 두 방향의 학습을 분리한다." },
    { name: "벡터 양자화", desc: "인코더 출력 z_e를 코드북의 모든 벡터와 L2 거리를 비교하여 가장 가까운 벡터로 교체한다. 이산적인 표현을 만드는 핵심 연산으로, CSI VQ 연구의 기반이 된다." }
  ],
  34: [
    { name: "AWQ 스케일링 변환", desc: "양자화 전에 중요한 가중치 채널(활성화가 큰 채널)을 스케일업하여 양자화 오차를 줄인다. 양자화 격자에서 중요 값이 더 정밀하게 표현되도록 하는 간단하면서도 효과적인 전처리이다." }
  ],
  35: [
    { name: "헤시안 민감도", desc: "손실 함수의 2차 미분(헤시안)으로 각 레이어가 양자화에 얼마나 민감한지를 측정한다. 헤시안 고유값이 크면 작은 가중치 변화에도 손실이 크게 변하므로, 높은 비트폭을 유지해야 한다." },
    { name: "ILP 비트 할당", desc: "정수 선형 프로그래밍(ILP)으로 레이어별 최적 비트폭을 자동 결정한다. 하드웨어 지연 시간 제약 하에서 전체 양자화 오차를 최소화하는 수학적으로 최적인 할당을 찾는다." }
  ],
  36: [
    { name: "OBQ 가중치 갱신", desc: "한 가중치를 양자화할 때 발생하는 오차를, 아직 양자화하지 않은 나머지 가중치를 헤시안 역행렬 정보로 보정하여 흡수한다. 한 가중치의 양자화가 다른 가중치에 미치는 영향을 최소화하는 2차 근사 방법이다." }
  ],
  37: [
    { name: "FSQ 양자화", desc: "각 잠재 차원 i를 L_i개의 정수 레벨로 단순 반올림한다. 전체 코드북 크기는 L₁ × L₂ × ... × L_d로 자동 결정되며, 별도 코드북 학습이 불필요하다. 구현이 매우 간단한 것이 최대 장점이다." }
  ],
  38: [
    { name: "잔차 벡터 양자화 (RVQ)", desc: "여러 단계의 VQ를 순차 적용하는 구조이다. 1단계: 잠재 벡터를 코드북으로 양자화. 2단계: 1단계 잔차(오차)를 다른 코드북으로 양자화. 이를 반복할수록 정밀해지며, 단계 수로 비트레이트를 조절한다." }
  ],
  39: [
    { name: "형상-이득 분해", desc: "잠재 벡터를 방향(단위 벡터 u, 형상)과 크기(스칼라 g, 이득)로 분리한다. 방향은 \"어떤 패턴인가\", 크기는 \"얼마나 강한가\"를 나타내며, 각각을 최적 방식으로 양자화할 수 있다." },
    { name: "그라스만 코드북", desc: "단위 벡터(형상)를 양자화하기 위한 코드북으로, 그라스만 다양체(방향만 다루는 수학적 공간) 위에서 벡터 간 최대 내적을 기준으로 설계된다. 방향 정보를 효율적으로 이산화한다." }
  ]
};

for (const [paperIdx, eqs] of Object.entries(eqUpdates)) {
  const paper = data.papers[parseInt(paperIdx)];
  if (paper.key_equations) {
    for (const update of eqs) {
      const eq = paper.key_equations.find(e => e.name === update.name);
      if (eq) {
        eq.description = update.desc;
      } else {
        console.warn(`  Warning: equation "${update.name}" not found in paper ${paperIdx}`);
      }
    }
  }
}

fs.writeFileSync('public/data/initial-papers.json', JSON.stringify(data, null, 2) + '\n', 'utf-8');
console.log('Updated all remaining equation descriptions (papers 6-39) to be learner-friendly.');
