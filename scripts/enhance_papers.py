import json
import sys

# Read the file
with open(r'C:\Users\hyunj\csiautoencoder\public\data\initial-papers.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

papers = data['papers']

# ============================================================
# Paper 0: CsiNet - ONLY add 4 new fields
# ============================================================
papers[0]['difficulty_level'] = 'beginner'
papers[0]['prerequisites'] = [
    "Massive MIMO 기본 개념 (안테나 배열, 빔포밍)",
    "CSI(Channel State Information)가 무엇이고 왜 필요한지",
    "오토인코더 기본 구조 (인코더-잠재공간-디코더)",
    "FDD vs TDD 차이점"
]
papers[0]['learning_objectives'] = [
    "CSI 피드백 문제를 왜 딥러닝으로 접근하는지 설명할 수 있다",
    "각도-지연 도메인 변환의 목적을 이해한다",
    "CsiNet의 인코더-디코더 구조와 RefineNet 역할을 설명할 수 있다",
    "압축비(γ)와 NMSE 간 트레이드오프를 이해한다"
]
papers[0]['self_check_questions'] = [
    "CsiNet은 왜 각도-지연 도메인에서 CSI를 처리하는가?",
    "압축비 γ=16일 때, 원본 1024차원 CSI는 몇 차원으로 압축되는가?",
    "RefineNet이 없으면 어떤 문제가 생기는가?",
    "기존 압축센싱(CS) 대비 CsiNet의 핵심 차이는 무엇인가?"
]

# ============================================================
# Paper 1: CsiNet-LSTM - difficulty: intermediate
# ============================================================
papers[1]['difficulty_level'] = 'intermediate'
papers[1]['prerequisites'] = [
    "CsiNet의 인코더-디코더 구조와 CSI 피드백 흐름",
    "LSTM(Long Short-Term Memory) 셀의 게이트 메커니즘 (입력/망각/출력 게이트)",
    "시계열 데이터에서 시간적 상관관계의 의미",
    "FDD Massive MIMO에서 시변 채널의 특성"
]
papers[1]['learning_objectives'] = [
    "시변 CSI에서 시간적 중복성을 활용하는 방식의 장점을 설명할 수 있다",
    "CsiNet과 CsiNet-LSTM의 구조적 차이를 비교할 수 있다",
    "LSTM 은닉 상태가 CSI 피드백에서 어떤 역할을 하는지 이해한다",
    "프레임 단위 독립 압축 대비 시간적 압축의 오버헤드 절감 원리를 파악한다"
]
papers[1]['self_check_questions'] = [
    "CsiNet-LSTM은 CsiNet과 비교해 어떤 추가 정보를 활용하는가?",
    "LSTM의 은닉 상태 h_{t-1}이 CSI 피드백에서 하는 역할은 무엇인가?",
    "사용자가 정지해 있을 때와 빠르게 이동할 때, CsiNet-LSTM의 이점은 어떻게 달라지는가?",
    "CsiNet-LSTM의 한계점은 무엇이며, 이를 개선하려면 어떤 방향이 있는가?"
]
papers[1]['abstract'] = "이 논문의 핵심 질문은 \"시간에 따라 변하는 CSI의 프레임 간 중복성을 활용하면, 매 프레임마다 전체 CSI를 새로 보내지 않아도 충분한 복원 품질을 얻을 수 있는가?\"이다.\n\nCsiNet은 한 시점의 CSI만 독립적으로 압축하므로, 연속된 프레임 사이의 유사성을 활용하지 못한다. CsiNet-LSTM은 LSTM 순환 구조를 인코더와 디코더에 통합하여, 이전 프레임의 기억(은닉 상태)을 현재 압축에 반영한다. 덕분에 이미 전송한 정보를 중복 전송하지 않아도 되어, 동일 품질에서 피드백 오버헤드를 크게 줄인다."
papers[1]['key_contributions'] = [
    "문제 정의: 기존 CsiNet이 각 프레임을 독립적으로 압축하여 시간적 중복 전송이 발생하는 비효율성을 지적했다.",
    "핵심 기법: LSTM 순환 레이어를 CNN 인코더/디코더에 통합하여, 이전 프레임의 은닉 상태를 현재 압축에 활용하는 시간적 압축 구조를 설계했다.",
    "설계 차별점: CsiNet의 공간 특징 추출(CNN)과 LSTM의 시간 기억력을 결합하여, 공간-시간 복합 정보를 하나의 파이프라인에서 처리한다.",
    "실험 검증: 시변 채널(사용자 이동 시나리오)에서 프레임 단위 CsiNet 대비 상당한 NMSE 개선을 달성하고, 다양한 이동 속도에서의 성능을 분석했다.",
    "실용성: 시변 환경에서 피드백 오버헤드를 줄이면서도 복원 품질을 유지하여, 실제 모바일 통신 환경에서의 적용 가능성을 높였다.",
    "연구사적 의미: CSI 피드백에 시간적 상관관계를 최초로 본격 활용한 연구로, 이후 MarkovNet, SLATE 등 시간축 활용 연구의 기초가 되었다."
]
papers[1]['key_equations'] = [
    {
        "name": "시간적 CSI 피드백",
        "latex": "\\mathbf{s}_t = f_{\\text{enc}}(\\mathbf{H}_{a,t}, \\mathbf{h}_{t-1})",
        "description": "해석: 인코더가 현재 CSI(H_{a,t})뿐 아니라 이전 시점의 은닉 상태(h_{t-1})도 함께 입력으로 받는다. h_{t-1}에는 이전 프레임들의 요약 정보가 담겨 있으므로, 인코더는 '이미 보낸 정보'와 '새로 변한 정보'를 구별할 수 있다. 그 결과, 중복되는 부분은 다시 전송하지 않아 코드워드 길이를 줄이면서도 복원 품질을 유지한다."
    },
    {
        "name": "LSTM 은닉 상태 갱신",
        "latex": "\\mathbf{h}_t = \\text{LSTM}(\\mathbf{s}_t, \\mathbf{h}_{t-1})",
        "description": "해석: LSTM 셀은 현재 코드워드(s_t)와 이전 기억(h_{t-1})을 결합하여 새로운 기억(h_t)을 생성한다. 입력 게이트는 새 정보 중 얼마나 저장할지, 망각 게이트는 과거 기억 중 얼마나 유지할지를 학습적으로 결정한다. 이 메커니즘 덕분에 빠르게 변하는 채널에서는 새 정보를 많이 반영하고, 느리게 변하는 채널에서는 과거 기억을 많이 재활용하는 적응적 동작이 가능하다."
    }
]
papers[1]['architecture_detail'] = """2.1 문제 설정
FDD Massive MIMO에서 UE는 매 시간 슬롯마다 CSI를 BS로 피드백해야 한다. 기존 CsiNet은 각 프레임을 독립적으로 압축하므로, 연속된 프레임 사이의 유사성(시간적 상관관계)을 전혀 활용하지 못한다.

2.2 기존 접근의 한계
CsiNet은 \"정지 사진\"만 보는 모델이다. 사용자가 천천히 이동하면 CSI는 프레임 간에 매우 유사한데도, 매번 전체를 새로 압축하여 불필요한 오버헤드가 발생한다.

2.3 CsiNet-LSTM 설계 철학
핵심 아이디어는 \"동영상처럼 연속 프레임을 함께 보자\"는 것이다. CNN이 각 프레임의 공간 특징을 추출한 뒤, LSTM이 \"이전에 이미 보낸 정보\"를 기억하고 현재 프레임에서 달라진 부분만 효율적으로 인코딩한다.

2.4 핵심 기법 상세
- 인코더: CsiNet의 Conv2D 레이어로 공간 특징 추출 후, LSTM 레이어가 시간적 문맥을 추가한다.
- 디코더: LSTM이 시간적 기억을 복원에 반영하고, DeConv 레이어가 CSI를 재구성한다.
- 은닉 상태(h_t): 과거 프레임들의 핵심 정보를 압축 저장하는 \"메모리\"로 작동한다.
- 게이트 메커니즘: 입력/망각/출력 게이트가 새 정보와 과거 기억의 비율을 자동 조절한다.

2.5 수식이 말하는 의미
- 시간적 피드백식: 현재 CSI와 과거 기억의 결합으로 코드워드 생성 → 중복 제거
- LSTM 갱신식: 적응적 기억 관리 → 채널 변화 속도에 맞는 유연한 압축

2.6 이 논문이 남긴 것
CsiNet-LSTM은 CSI 피드백에 시간축 활용이라는 새로운 차원을 열었다. 이후 MarkovNet(차분 인코딩), SLATE(SwinLSTM) 등 시간적 상관관계를 활용하는 후속 연구들의 출발점이 되었다."""

# ============================================================
# Paper 2: CRNet - difficulty: intermediate
# ============================================================
papers[2]['difficulty_level'] = 'intermediate'
papers[2]['prerequisites'] = [
    "CsiNet의 인코더-디코더 구조와 각도-지연 도메인 변환",
    "다중 스케일 특징 추출의 개념 (Inception 모듈 등)",
    "Squeeze-and-Excitation(SE) 채널 어텐션의 원리",
    "잔차 연결(Residual Connection)의 역할과 효과"
]
papers[2]['learning_objectives'] = [
    "다중 해상도 특징 추출이 CSI 복원에 왜 유리한지 설명할 수 있다",
    "CRBlock의 병렬 합성곱 분기와 채널 어텐션 결합 구조를 이해한다",
    "CsiNet 대비 CRNet의 성능 향상 원인을 분석할 수 있다",
    "SE 블록이 특징 재보정에서 하는 역할을 설명할 수 있다"
]
papers[2]['self_check_questions'] = [
    "CRBlock에서 여러 크기의 합성곱 커널을 병렬로 사용하는 이유는 무엇인가?",
    "채널 어텐션(SE 블록)이 없으면 CRNet의 성능은 어떻게 변하겠는가?",
    "CRNet의 디코더가 여러 CRBlock을 쌓는 것은 RefineNet과 어떤 점에서 유사하고 다른가?",
    "CRNet이 CsiNet보다 계산량은 더 많은데 실용적인 이유는 무엇인가?"
]
papers[2]['abstract'] = "이 논문의 핵심 질문은 \"CSI 행렬에 존재하는 다양한 스케일의 패턴을 동시에 포착하면, 단일 커널 크기 합성곱보다 더 나은 복원이 가능한가?\"이다.\n\nCsiNet의 RefineNet은 고정된 3×3 커널만 사용하여, 넓은 범위의 채널 상관관계를 놓칠 수 있다. CRNet은 CRBlock에서 3×3, 5×5, 7×7 등 여러 크기의 합성곱을 병렬로 적용하고, Squeeze-and-Excitation 채널 어텐션으로 각 스케일의 기여도를 자동 조절하여 CSI 복원 정확도를 크게 높였다."
papers[2]['key_contributions'] = [
    "문제 정의: CsiNet의 단일 커널 크기 합성곱이 CSI 행렬의 다양한 스케일 패턴을 충분히 포착하지 못하는 한계를 지적했다.",
    "핵심 기법: 서로 다른 크기의 합성곱 필터(3×3, 5×5, 7×7)를 병렬로 적용하여 다중 해상도에서 동시에 특징을 추출하는 CRBlock을 설계했다.",
    "설계 차별점: Squeeze-and-Excitation(SE) 채널 어텐션을 CRBlock에 통합하여, 각 스케일 분기의 기여도를 데이터 기반으로 적응적으로 가중하는 구조를 채택했다.",
    "실험 검증: COST2100 실내/실외 데이터셋에서 다양한 압축비(γ=4~64)에 걸쳐 CsiNet 대비 일관된 NMSE 개선을 달성했다.",
    "실용성: 계산량 증가가 합리적인 수준이며, CRBlock의 모듈형 설계 덕분에 기존 아키텍처에 쉽게 통합할 수 있다.",
    "연구사적 의미: 다중 해상도 + 채널 어텐션 조합의 효과를 CSI 피드백에서 검증하여, ACRNet, CLNet 등 후속 연구의 기반이 되었다."
]
papers[2]['key_equations'] = [
    {
        "name": "다중 해상도 특징 집계",
        "latex": "\\mathbf{F}_{\\text{out}} = \\sum_{i=1}^{K} \\alpha_i \\cdot \\text{Conv}_{k_i}(\\mathbf{F}_{\\text{in}})",
        "description": "해석: 입력 특징맵 F_in에 서로 다른 크기의 합성곱 필터(k₁=3, k₂=5, k₃=7 등)를 각각 적용한다. 작은 필터는 인접 안테나/부반송파 간 세부 패턴을, 큰 필터는 넓은 범위의 채널 상관관계를 포착한다. 각 분기의 결과에 어텐션 가중치 α를 곱해 합산하므로, 네트워크가 현재 CSI에 가장 유용한 스케일을 자동으로 선택한다."
    },
    {
        "name": "채널 주의 (SE 블록)",
        "latex": "\\mathbf{\\alpha} = \\sigma(\\mathbf{W}_2 \\cdot \\delta(\\mathbf{W}_1 \\cdot \\text{GAP}(\\mathbf{F})))",
        "description": "해석: 전역 평균 풀링(GAP)으로 각 채널의 전체 공간 정보를 하나의 스칼라로 요약한다. 이 요약 벡터가 두 개의 FC 레이어(W₁으로 축소, W₂로 복원)와 시그모이드(σ)를 통과하면 0~1 사이의 채널별 가중치가 된다. 결과적으로 '유용한 채널은 강조, 불필요한 채널은 억제'하는 재보정(recalibration) 효과를 낸다."
    }
]
papers[2]['architecture_detail'] = """2.1 문제 설정
CsiNet이 CSI 피드백에 딥러닝을 도입했지만, 복원 정확도에는 아직 개선 여지가 있다. 특히 디코더의 RefineNet이 고정 크기 합성곱만 사용하여 다양한 스케일의 채널 패턴을 충분히 활용하지 못한다.

2.2 기존 접근의 한계
CsiNet의 RefineNet은 3×3 합성곱만 사용하므로, 수용 영역(receptive field)이 제한적이다. CSI 행렬에는 인접 안테나 간 지역적 상관관계뿐 아니라, 먼 안테나 간 넓은 범위의 상관관계도 존재하는데, 단일 커널로는 이를 동시에 포착하기 어렵다.

2.3 CRNet 설계 철학
\"여러 크기의 돋보기로 동시에 보자\"가 핵심 아이디어이다. 하나의 CRBlock 안에 3×3, 5×5, 7×7 병렬 합성곱 분기를 두고, 채널 어텐션(SE)으로 각 분기의 기여도를 자동 조절한다.

2.4 핵심 기법 상세
- CRBlock: 병렬 다중 스케일 합성곱 → SE 채널 어텐션 → 잔차 연결의 3단계 구조
- SE 채널 어텐션: GAP → FC(축소) → ReLU → FC(복원) → Sigmoid로 채널별 가중치 생성
- 인코더: Conv2D + FC (CsiNet과 동일한 경량 구조)
- 디코더: FC + 여러 CRBlock 적층 (품질 향상의 핵심)
- 잔차 연결: 각 CRBlock의 입출력을 더하여 안정적 학습 보장

2.5 수식이 말하는 의미
- 다중 해상도 집계식: 다양한 스케일의 패턴을 동시 포착 → 풍부한 특징 표현
- SE 블록식: 채널별 중요도 자동 재조정 → 유용한 정보 강조, 잡음 억제

2.6 이 논문이 남긴 것
CRNet은 CSI 피드백 디코더 설계의 새로운 기준을 세웠다. 다중 해상도 + 어텐션 조합의 효과를 검증하여, ACRNet(교차 도메인 확장), CLNet(경량화 변형) 등 다수의 후속 연구에 직접적 영향을 미쳤다."""

# ============================================================
# Paper 3: CLNet - difficulty: intermediate
# ============================================================
papers[3]['difficulty_level'] = 'intermediate'
papers[3]['prerequisites'] = [
    "CRNet의 다중 해상도 CRBlock 구조",
    "깊이별 분리 합성곱(Depthwise Separable Conv)의 원리와 파라미터 절감 효과",
    "모바일넷(MobileNet) 계열의 경량 네트워크 설계 원칙",
    "모델 경량화의 필요성 (UE 측 제약 조건)"
]
papers[3]['learning_objectives'] = [
    "깊이별 분리 합성곱이 표준 합성곱 대비 어떻게 파라미터를 줄이는지 수식으로 설명할 수 있다",
    "CLNet이 CRNet 수준의 성능을 유지하면서 경량화를 달성하는 원리를 이해한다",
    "AnciNet 보조 정제 모듈의 역할을 설명할 수 있다",
    "경량화와 성능 간 트레이드오프를 분석할 수 있다"
]
papers[3]['self_check_questions'] = [
    "깊이별 분리 합성곱(DW+PW)이 표준 합성곱 대비 파라미터를 약 1/K² 수준으로 줄이는 원리는?",
    "AnciNet을 제거하면 CLNet 성능이 어떻게 변하겠는가?",
    "CLNet이 CRNet보다 파라미터가 적은데도 유사한 NMSE를 달성할 수 있는 이유는?",
    "UE 측 인코더를 경량화하는 것이 왜 디코더 경량화보다 중요한가?"
]
papers[3]['abstract'] = "이 논문의 핵심 질문은 \"CRNet 수준의 CSI 복원 성능을 유지하면서도, UE(사용자 단말)에 배포 가능한 수준으로 계산량과 파라미터를 줄일 수 있는가?\"이다.\n\n기존 CRNet은 다중 해상도 합성곱과 채널 어텐션으로 높은 복원 성능을 달성했지만, 모바일 기기에서 실행하기에는 계산량과 파라미터가 부담스럽다. CLNet은 표준 합성곱을 깊이별 분리 합성곱(Depthwise Separable Conv)으로 대체하여 파라미터를 약 1/K 수준으로 줄이고, 보조 정제 네트워크(AnciNet)로 경량화에 따른 성능 손실을 보완한다."
papers[3]['key_contributions'] = [
    "문제 정의: CRNet의 높은 복원 성능이 모바일 UE의 제한된 계산 자원에서는 실행 불가능한 수준임을 분석했다.",
    "핵심 기법: 표준 합성곱을 깊이별(Depthwise) + 포인트와이즈(Pointwise) 분리 합성곱으로 대체하여, 파라미터와 FLOPs를 약 1/K 수준으로 감소시켰다.",
    "설계 차별점: 경량화로 인한 성능 저하를 보완하기 위해 AnciNet 보조 정제 모듈을 도입하고, CSI의 위상 정보를 보존하는 복소수 처리 경로를 추가했다.",
    "실험 검증: CRNet과 유사한 NMSE를 달성하면서 파라미터 수를 약 1/K로 줄임을 COST2100 데이터셋에서 입증했다.",
    "실용성: 모바일 기기에서의 실시간 CSI 피드백이 가능한 수준의 경량 네트워크를 실현했다.",
    "연구사적 의미: 모바일넷 계열의 경량화 기법을 CSI 피드백에 체계적으로 적용한 최초의 연구로, 이후 ShuffleCsiNet 등 경량 아키텍처 연구에 영감을 주었다."
]
papers[3]['key_equations'] = [
    {
        "name": "깊이별 분리 합성곱",
        "latex": "\\text{DSConv}(\\mathbf{X}) = \\text{PW}_{1\\times1}(\\text{DW}_{k\\times k}(\\mathbf{X}))",
        "description": "해석: 일반 합성곱을 2단계로 분해하여 파라미터를 대폭 줄이는 핵심 연산이다. 1단계(DW): 각 입력 채널을 독립적으로 k×k 합성곱하여 공간 패턴을 추출한다. 2단계(PW): 1×1 합성곱으로 채널 간 정보를 혼합한다. 일반 합성곱이 k²×C_in×C_out 파라미터를 사용하는 반면, 분리 합성곱은 k²×C_in + C_in×C_out으로 약 1/C_out 또는 1/k² 수준으로 줄어든다."
    },
    {
        "name": "CLNet 복잡도 감소",
        "latex": "\\frac{\\text{Params}_{\\text{CLNet}}}{\\text{Params}_{\\text{CRNet}}} \\approx \\frac{1}{K}",
        "description": "해석: CLNet이 CRNet 대비 약 1/K의 파라미터만 사용함을 나타내는 비율이다. K는 합성곱 커널 크기 관련 상수로, 분리 합성곱의 경량화 효과를 정량적으로 보여준다. 예를 들어 K=9(3×3 커널 기준)이면 파라미터가 약 1/9로 줄어든다. 이 감소에도 불구하고 AnciNet 보조 모듈이 성능을 보완하여 유사한 NMSE를 유지한다."
    }
]
papers[3]['architecture_detail'] = """2.1 문제 설정
CRNet이 높은 복원 품질을 보였지만, 모바일 UE에서 실시간으로 실행하기에는 파라미터와 FLOPs가 여전히 크다. 실제 배포를 위해서는 UE 측 인코더뿐 아니라 네트워크 전체의 경량화가 필요하다.

2.2 기존 접근의 한계
CRNet의 CRBlock은 여러 크기의 표준 합성곱을 병렬로 사용하므로 파라미터가 많다. 특히 5×5, 7×7 커널은 3×3 대비 각각 약 2.8배, 5.4배의 파라미터를 요구한다.

2.3 CLNet 설계 철학
핵심 전략은 \"같은 구조를 유지하되, 무거운 부품을 가벼운 부품으로 교체\"하는 것이다. 모바일넷에서 검증된 깊이별 분리 합성곱(DSConv)을 도입하여, CRNet의 다중 해상도 아이디어는 살리면서 계산량을 대폭 줄인다.

2.4 핵심 기법 상세
- 깊이별 합성곱(DW): 각 채널을 독립적으로 k×k 합성곱 → 공간 패턴 추출
- 포인트와이즈 합성곱(PW): 1×1 합성곱으로 채널 간 정보 혼합 → 채널 상호작용
- AnciNet: 경량화에 따른 미세한 성능 저하를 보완하는 보조 정제 네트워크
- 복소수 경로: CSI의 실수부와 허수부(위상 정보)를 별도로 처리하여 위상 보존

2.5 수식이 말하는 의미
- DSConv식: 공간 처리와 채널 혼합을 분리 → 파라미터 약 1/K² 절감
- 복잡도 비율식: CRNet 대비 정량적 파라미터 감소량을 보여주는 효율성 지표

2.6 이 논문이 남긴 것
CLNet은 \"성능과 경량성의 균형\"이라는 실용적 관점을 CSI 피드백 연구에 도입했다. 이후 ShuffleCsiNet(그룹 합성곱 활용), ENet(비대칭 설계) 등 경량 아키텍처 연구의 직접적 동기가 되었다."""

# ============================================================
# Paper 4: TransNet - difficulty: intermediate
# ============================================================
papers[4]['difficulty_level'] = 'intermediate'
papers[4]['prerequisites'] = [
    "Transformer의 셀프 어텐션 메커니즘 (Query, Key, Value)",
    "CNN의 수용 영역(Receptive Field) 제한과 그 의미",
    "위치 인코딩(Positional Encoding)의 역할",
    "CsiNet/CRNet의 CNN 기반 CSI 피드백 구조"
]
papers[4]['learning_objectives'] = [
    "CNN 기반 CSI 피드백의 수용 영역 제한이 왜 문제가 되는지 설명할 수 있다",
    "셀프 어텐션이 장거리 공간 의존성을 포착하는 원리를 이해한다",
    "TransNet의 Multi-Head Self-Attention 구조를 설명할 수 있다",
    "CNN 대비 Transformer 기반 CSI 피드백의 장단점을 비교할 수 있다"
]
papers[4]['self_check_questions'] = [
    "CNN의 3×3 합성곱이 CSI 행렬의 장거리 의존성을 포착하기 어려운 이유는?",
    "Multi-Head Attention에서 헤드 수(h)를 늘리면 어떤 장점과 단점이 있는가?",
    "TransNet이 낮은 압축비에서 특히 강한 성능을 보이는 이유는 무엇인가?",
    "위치 인코딩이 없으면 TransNet은 어떤 정보를 잃게 되는가?"
]
papers[4]['abstract'] = "이 논문의 핵심 질문은 \"CNN의 제한된 수용 영역 대신 Transformer의 전역 어텐션을 사용하면, CSI 행렬 내 장거리 공간 의존성을 더 효과적으로 포착하여 복원 품질을 높일 수 있는가?\"이다.\n\nCNN 기반 CSI 피드백 모델(CsiNet, CRNet)은 합성곱 커널의 제한된 수용 영역 때문에 먼 위치 간 관계를 직접 포착하기 어렵다. TransNet은 Multi-Head Self-Attention을 전면 도입하여 CSI 행렬의 모든 위치 간 관계를 한 번에 계산하고, 특히 낮은 압축비에서 최첨단 NMSE 성능을 달성했다."
papers[4]['key_contributions'] = [
    "문제 정의: CNN 합성곱의 지역적 수용 영역이 CSI 행렬 내 먼 안테나/부반송파 간 장거리 상관관계를 포착하지 못하는 구조적 한계를 지적했다.",
    "핵심 기법: CSI 피드백에 Multi-Head Self-Attention을 최초로 전면 적용하여, CSI 행렬의 모든 위치 쌍 간 전역 의존성을 학습하는 구조를 도입했다.",
    "설계 차별점: CSI 행렬을 패치로 분할하고 위치 인코딩을 추가하는 Transformer 전처리를 설계하여, 합성곱 없이도 공간적 구조 정보를 보존했다.",
    "실험 검증: COST2100 데이터셋에서 낮은 압축비(γ=4, 8)에서 CsiNet, CRNet을 능가하는 NMSE를 달성하고, 높은 압축비에서도 경쟁력 있는 성능을 보였다.",
    "실용성: 전역 어텐션의 계산 비용이 높지만, 병렬 연산에 최적화된 하드웨어(GPU)에서는 CNN과 유사한 처리 속도를 달성할 수 있다.",
    "연구사적 의미: CSI 피드백 연구에 Transformer 패러다임을 도입하여, 이후 CSI-GPT, SemCSINet, WiFo-CF 등 Transformer 기반 후속 연구의 시발점이 되었다."
]
papers[4]['key_equations'] = [
    {
        "name": "다중 헤드 셀프 어텐션",
        "latex": "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}",
        "description": "해석: CSI 패치들 사이의 관계를 계산하는 핵심 연산이다. 각 패치에서 Query(질문), Key(열쇠), Value(값)를 만들고, Q와 K의 내적으로 '이 두 위치가 얼마나 관련 있는지' 유사도 점수를 계산한다. √d_k로 나누는 것은 차원이 커질 때 점수가 극단적으로 커지는 것을 방지하기 위함이다. Softmax로 정규화된 가중치로 V를 합산하면, 먼 위치의 관련 정보도 직접 반영된다."
    },
    {
        "name": "다중 헤드 출력",
        "latex": "\\text{MHA}(\\mathbf{X}) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)\\mathbf{W}^O",
        "description": "해석: 서로 다른 관점으로 어텐션을 h번 수행한 결과를 이어붙인(Concat) 후, 출력 가중치 W^O로 최종 변환한다. 하나의 어텐션 헤드는 하나의 관계 패턴만 포착하지만, 여러 헤드를 사용하면 '인접 안테나 상관', '먼 부반송파 상관' 등 다양한 유형의 의존성을 동시에 학습할 수 있다."
    }
]
papers[4]['architecture_detail'] = """2.1 문제 설정
FDD Massive MIMO에서 CSI 행렬은 안테나 축과 부반송파 축을 가진 2D 구조이다. 이 행렬 내에서 먼 위치 간에도 물리적 채널 특성에 의한 상관관계가 존재한다.

2.2 기존 접근의 한계
CNN은 고정 크기 커널(예: 3×3)의 수용 영역 내에서만 패턴을 학습한다. 멀리 떨어진 위치 간 상관관계를 포착하려면 많은 레이어를 쌓아야 하며, 그래도 간접적인 전파에 의존한다.

2.3 TransNet 설계 철학
\"모든 위치 간 관계를 한 번에 직접 계산하자\"가 핵심이다. CNN의 지역 합성곱 대신 Transformer의 Self-Attention을 전면 도입하여, CSI 행렬의 모든 위치 쌍 간 유사도를 직접 계산한다.

2.4 핵심 기법 상세
- 패치 분할: CSI 행렬을 작은 패치(조각)로 나누어 Transformer의 토큰(입력 단위)으로 사용
- 위치 인코딩: 패치의 공간적 위치 정보를 학습 가능한 임베딩으로 추가
- Multi-Head Self-Attention: h개의 독립적 어텐션 헤드가 서로 다른 유형의 공간 의존성을 동시 학습
- 인코더: Self-Attention 출력 → FC 압축으로 코드워드 생성
- 디코더: FC 복원 → Multi-Head Attention으로 전역 의존성 재구성

2.5 수식이 말하는 의미
- Self-Attention식: 모든 위치 쌍의 유사도 계산 → 장거리 의존성 직접 포착
- Multi-Head 출력식: 다양한 관계 패턴의 병렬 학습 → 풍부한 표현력

2.6 이 논문이 남긴 것
TransNet은 CSI 피드백 연구에 Transformer 패러다임을 최초로 도입했다. CNN에서 Transformer로의 아키텍처 전환을 촉발하여, CSI-GPT(생성형), SemCSINet(의미적), WiFo-CF(파운데이션 모델) 등의 연구가 이 위에서 발전했다."""

# ============================================================
# Paper 5: ACRNet - difficulty: intermediate
# ============================================================
papers[5]['difficulty_level'] = 'intermediate'
papers[5]['prerequisites'] = [
    "CRNet의 다중 해상도 CRBlock 구조와 채널 어텐션",
    "공간 도메인과 주파수 도메인에서의 CSI 표현 차이",
    "교차 도메인(Cross-Domain) 특징 융합의 개념",
    "하다마드 곱(원소별 곱)의 의미와 활용"
]
papers[5]['learning_objectives'] = [
    "공간 도메인과 주파수 도메인에서 CSI를 동시에 처리하는 이점을 설명할 수 있다",
    "AggregationBlock의 다중 스케일 특징 융합 구조를 이해한다",
    "교차 도메인 어텐션이 특징 재조정에서 하는 역할을 설명할 수 있다",
    "CRNet 대비 ACRNet의 효율성과 성능 향상 원인을 분석할 수 있다"
]
papers[5]['self_check_questions'] = [
    "ACRNet이 공간 도메인과 주파수 도메인을 동시에 활용하는 이유는 무엇인가?",
    "AggregationBlock에서 concat + 1×1 Conv 구조가 SE 블록 대비 어떤 장점이 있는가?",
    "교차 도메인 어텐션에서 하다마드 곱은 어떤 역할을 하는가?",
    "ACRNet이 CRNet보다 적은 파라미터로 더 좋은 성능을 내는 핵심 원인은?"
]
papers[5]['abstract'] = "이 논문의 핵심 질문은 \"CSI 행렬을 공간 도메인과 주파수 도메인에서 동시에 분석하고 두 시각의 정보를 교차 융합하면, 단일 도메인 처리보다 더 정확한 복원이 가능한가?\"이다.\n\nCRNet은 다중 해상도 합성곱으로 공간적 특징을 풍부하게 추출했지만, 주파수 도메인의 보완적 정보를 활용하지 않았다. ACRNet은 AggregationBlock에서 3×3, 5×5, 7×7 커널을 concat 방식으로 통합하고, 교차 도메인 어텐션 모듈로 공간-주파수 특징 간 상호작용을 학습하여, CRNet보다 적은 파라미터로 더 높은 복원 정확도를 달성한다."
papers[5]['key_contributions'] = [
    "문제 정의: CRNet이 공간 도메인의 다중 해상도 특징만 활용하고, 주파수 도메인의 보완적 정보를 간과하는 한계를 지적했다.",
    "핵심 기법: 공간 도메인과 주파수 도메인의 특징을 교차 어텐션으로 융합하는 AggregationBlock을 설계하여, 두 도메인의 상호보완적 정보를 동시에 활용한다.",
    "설계 차별점: CRNet의 SE 기반 가중합 대신 concat + 1×1 Conv + 잔차 연결 구조를 채택하여, 정보 손실 없이 다중 스케일 특징을 통합한다.",
    "실험 검증: COST2100 실내/실외 데이터셋에서 CRNet 및 CLNet을 모든 압축비에서 능가하는 NMSE를 달성했다.",
    "실용성: CRNet보다 적은 파라미터로 더 높은 성능을 내어, 효율성과 정확도의 파레토 최적에 근접했다.",
    "연구사적 의미: 교차 도메인 특징 융합이라는 새로운 설계 원칙을 CSI 피드백에 도입하여, AiANet 등 후속 어텐션 기반 연구에 직접적 영향을 미쳤다."
]
papers[5]['key_equations'] = [
    {
        "name": "교차 도메인 특징 융합",
        "latex": "\\mathbf{F}_{\\text{fused}} = \\text{Agg}(\\mathbf{F}_{\\text{spatial}}, \\mathbf{F}_{\\text{freq}}) = \\mathbf{F}_{\\text{spatial}} \\odot \\mathbf{A}_{\\text{cross}} + \\mathbf{F}_{\\text{freq}}",
        "description": "해석: 공간 도메인 특징(F_spatial)과 주파수 도메인 특징(F_freq)을 결합하는 핵심 연산이다. 교차 어텐션 맵(A_cross)은 주파수 특징의 관점에서 공간 특징의 어떤 부분이 중요한지를 판단하여 가중치를 생성한다. 이 가중치를 공간 특징에 원소별로 곱한(⊙) 뒤, 주파수 특징을 더하면 두 도메인의 정보가 상호보완적으로 결합된다."
    },
    {
        "name": "집계 블록",
        "latex": "\\mathbf{F}_{\\text{out}} = \\text{Conv}(\\text{Cat}(\\mathbf{F}_{3\\times3}, \\mathbf{F}_{5\\times5}, \\mathbf{F}_{7\\times7})) + \\mathbf{F}_{\\text{in}}",
        "description": "해석: 3×3, 5×5, 7×7 세 가지 크기의 합성곱 결과를 채널 축으로 이어붙이고(Cat), 1×1 합성곱으로 채널 수를 원래대로 줄인 뒤, 입력과 더하는(잔차 연결) 구조이다. CRNet의 SE 기반 가중합과 달리, concat 방식은 각 스케일의 정보를 모두 보존한 상태에서 1×1 Conv가 최적 조합을 학습하므로 정보 손실이 적다."
    }
]
papers[5]['architecture_detail'] = """2.1 문제 설정
CSI 행렬은 공간(안테나) 축과 주파수(부반송파) 축을 가지며, 각 축에서 서로 다른 유형의 상관관계가 존재한다. 이 두 도메인의 정보를 모두 활용하면 더 정확한 복원이 가능하다.

2.2 기존 접근의 한계
CRNet은 다중 해상도 합성곱으로 공간적 특징을 풍부하게 추출하지만, 주파수 도메인의 보완적 정보를 명시적으로 활용하지 않는다. 또한 SE 기반 가중합은 각 스케일을 스칼라로 요약하여 세밀한 정보가 손실될 수 있다.

2.3 ACRNet 설계 철학
\"공간과 주파수, 두 가지 시각으로 동시에 보자\"가 핵심 아이디어이다. AggregationBlock으로 다중 스케일 특징을 concat 방식으로 통합하고, 교차 도메인 어텐션으로 공간-주파수 간 상호작용을 명시적으로 학습한다.

2.4 핵심 기법 상세
- AggregationBlock: 3×3·5×5·7×7 커널 결과를 concat → 1×1 Conv로 채널 조정 → 잔차 연결
- 교차 도메인 어텐션: 주파수 특징으로부터 공간 특징의 중요도 맵을 생성하여 하다마드 곱으로 재조정
- 인코더: Conv2D + FC (경량 구조 유지)
- 디코더: FC + AggregationBlock 적층 + 교차 도메인 모듈

2.5 수식이 말하는 의미
- 교차 도메인 융합식: 두 도메인의 상호보완적 결합 → 단일 도메인 대비 풍부한 정보
- 집계 블록식: concat + 잔차 연결 → 정보 보존과 안정적 학습의 동시 달성

2.6 이 논문이 남긴 것
ACRNet은 교차 도메인 특징 융합이라는 새로운 설계 원칙을 CSI 피드백에 도입했다. CRNet의 다중 해상도 아이디어를 계승하면서도 더 효율적인 구조로 발전시켜, AiANet이 ACRNet을 기준선으로 삼을 만큼 강력한 벤치마크가 되었다."""

# ============================================================
# Paper 6: DS-NLCsiNet - difficulty: intermediate
# ============================================================
papers[6]['difficulty_level'] = 'intermediate'
papers[6]['prerequisites'] = [
    "CsiNet의 인코더-디코더 구조와 RefineNet",
    "비국소(Non-Local) 연산의 개념 (모든 위치 쌍의 유사도 계산)",
    "깊이별 분리 합성곱(Depthwise Separable Conv)의 원리",
    "CNN의 수용 영역 제한과 장거리 의존성 문제"
]
papers[6]['learning_objectives'] = [
    "비국소(Non-Local) 블록이 장거리 의존성을 포착하는 원리를 설명할 수 있다",
    "깊이별 분리 합성곱과 비국소 블록의 역할 분담을 이해한다",
    "임베디드 가우시안 유사도 함수의 작동 방식을 설명할 수 있다",
    "CsiNet/CRNet 대비 DS-NLCsiNet의 NMSE-FLOPs 트레이드오프 개선을 분석할 수 있다"
]
papers[6]['self_check_questions'] = [
    "비국소 블록과 Transformer의 셀프 어텐션은 어떤 점에서 유사하고 다른가?",
    "비국소 블록의 계산 복잡도가 O(N²)인 이유는 무엇이며, 이것이 실용성에 미치는 영향은?",
    "깊이별 분리 합성곱 없이 표준 합성곱만 사용하면 DS-NLCsiNet의 효율성은 어떻게 변하는가?",
    "비국소 블록의 정규화 상수 C(x)는 왜 필요한가?"
]
papers[6]['abstract'] = "이 논문의 핵심 질문은 \"CNN의 지역적 합성곱으로는 놓치는 장거리 공간 의존성을 비국소(Non-Local) 연산으로 포착하면서도, 계산 비용을 낮게 유지할 수 있는가?\"이다.\n\nCsiNet과 CRNet은 합성곱 커널의 제한된 수용 영역 때문에 CSI 행렬 내 먼 위치 간 관계를 직접 포착하지 못한다. DS-NLCsiNet은 비국소 블록으로 모든 위치 쌍의 유사도를 계산하여 장거리 의존성을 포착하고, 깊이별 분리 합성곱(DS Conv)으로 나머지 연산의 계산 비용을 줄여서, 향상된 NMSE-FLOPs 트레이드오프를 달성한다."
papers[6]['key_contributions'] = [
    "문제 정의: CsiNet과 CRNet의 CNN 합성곱이 CSI 행렬 내 먼 안테나/부반송파 간 장거리 상관관계를 놓치는 구조적 한계를 분석했다.",
    "핵심 기법: 비국소(Non-Local) 어텐션 블록을 도입하여 CSI 행렬의 모든 위치 쌍 간 유사도를 직접 계산하고, 장거리 의존성을 포착하는 구조를 설계했다.",
    "설계 차별점: 비국소 블록의 '넓은 시야'와 깊이별 분리 합성곱의 '가벼운 계산'을 분업시켜, 성능 향상과 경량화를 동시에 추구했다.",
    "실험 검증: COST2100 데이터셋에서 CsiNet 및 CRNet 대비 향상된 NMSE-FLOPs 트레이드오프를 달성하여, 적은 계산량으로 더 좋은 복원 품질을 보였다.",
    "실용성: 비국소 블록의 O(N²) 복잡도를 깊이별 분리 합성곱으로 보완하여, 전체적으로 합리적인 계산 비용을 유지한다.",
    "연구사적 의미: CNN 기반 CSI 피드백에 비국소 연산을 도입한 최초의 연구로, Transformer 기반 TransNet과 함께 '장거리 의존성' 포착의 중요성을 입증했다."
]
papers[6]['key_equations'] = [
    {
        "name": "비국소 블록",
        "latex": "\\mathbf{y}_i = \\frac{1}{C(\\mathbf{x})} \\sum_{\\forall j} f(\\mathbf{x}_i, \\mathbf{x}_j) g(\\mathbf{x}_j)",
        "description": "해석: 특정 위치 i의 출력을 계산할 때, 가까운 이웃뿐 아니라 모든 위치 j의 정보를 가중합한다. 가중치 f(x_i, x_j)는 두 위치가 얼마나 유사한지를 측정하고, g(x_j)는 위치 j의 정보를 변환한다. C(x)는 정규화 상수로, 가중치의 합이 적절한 범위가 되도록 조절한다. CNN과 달리 거리에 관계없이 관련된 모든 위치의 정보를 직접 참조할 수 있다."
    },
    {
        "name": "임베디드 가우시안 유사도",
        "latex": "f(\\mathbf{x}_i, \\mathbf{x}_j) = e^{\\theta(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j)}",
        "description": "해석: 두 위치의 유사도를 측정하는 함수이다. 각 위치를 학습된 임베딩(θ와 φ)으로 변환한 뒤 내적을 취하고, 지수 함수를 적용한다. 소프트맥스와 결합하면 Transformer의 어텐션과 수학적으로 동일한 형태가 된다. 학습 가능한 임베딩 덕분에 단순 거리가 아닌 의미적 유사도를 기준으로 관련 위치를 찾을 수 있다."
    }
]
papers[6]['architecture_detail'] = """2.1 문제 설정
CSI 행렬에서 물리적으로 먼 안테나나 부반송파 사이에도 채널 특성에 의한 상관관계가 존재한다. 이 장거리 의존성을 포착하면 복원 정확도를 높일 수 있다.

2.2 기존 접근의 한계
CsiNet의 3×3 합성곱은 9개 이웃만 참조하고, CRNet의 7×7도 49개 이웃까지만 확장된다. 멀리 떨어진 위치의 정보를 활용하려면 많은 레이어를 쌓아야 하며, 이는 파라미터와 계산량을 증가시킨다.

2.3 DS-NLCsiNet 설계 철학
\"넓은 시야\"는 비국소 블록이, \"가벼운 계산\"은 깊이별 분리 합성곱이 각각 담당하는 분업 구조를 채택했다. 두 기법의 상호보완적 조합으로 성능과 효율을 동시에 개선한다.

2.4 핵심 기법 상세
- 비국소(Non-Local) 블록: 모든 위치 쌍의 임베디드 가우시안 유사도를 계산하여 장거리 의존성 포착
- 깊이별 분리 합성곱(DS Conv): 지역적 패턴 추출을 담당하면서 파라미터 절감
- 분업 구조: Non-Local → 전역 관계 학습, DS Conv → 지역 패턴 추출 + 계산량 절감
- 잔차 연결: 비국소 블록의 출력을 입력에 더하여 학습 안정성 확보

2.5 수식이 말하는 의미
- 비국소 블록식: 거리 무관하게 모든 위치의 정보를 가중합 → 장거리 의존성 직접 포착
- 임베디드 가우시안식: 학습 가능한 유사도 함수 → 의미적으로 관련된 위치를 자동 발견

2.6 이 논문이 남긴 것
DS-NLCsiNet은 CNN 프레임워크 안에서 장거리 의존성 문제를 해결하는 실용적 방법을 제시했다. TransNet이 Transformer로 완전 전환한 것과 달리, 기존 CNN 구조에 비국소 블록을 플러그인으로 추가하는 접근법을 보여주어, 기존 모델의 점진적 개선에 활용할 수 있는 경로를 열었다."""

# ============================================================
# Paper 8: Quantization of DNNs for CSI - difficulty: intermediate
# ============================================================
papers[8]['difficulty_level'] = 'intermediate'
papers[8]['prerequisites'] = [
    "CsiNet/CRNet의 코드워드 피드백 메커니즘",
    "양자화의 기본 개념 (연속값을 이산값으로 변환)",
    "Straight-Through Estimator(STE)의 역할",
    "부동소수점(FP32) vs 고정소수점(INT) 표현의 차이"
]
papers[8]['learning_objectives'] = [
    "부동소수점 코드워드를 유한 비트로 양자화해야 하는 실용적 이유를 설명할 수 있다",
    "양자화 인식 학습(QAT)이 사후 양자화보다 성능이 좋은 원리를 이해한다",
    "STE가 양자화의 미분 불가능 문제를 어떻게 우회하는지 설명할 수 있다",
    "총 피드백 비트 수와 압축비의 관계를 계산할 수 있다"
]
papers[8]['self_check_questions'] = [
    "학습 없이 사후 양자화를 적용하면 왜 성능이 크게 떨어지는가?",
    "STE에서 순방향은 양자화를 적용하고 역방향은 항등 함수를 쓰는 이유는?",
    "4비트 양자화와 8비트 양자화의 NMSE 차이는 얼마나 되며, 그 원인은?",
    "총 피드백 비트 수 B_total = M × B에서, M과 B 중 어느 것을 줄이는 것이 더 효과적인가?"
]
papers[8]['abstract'] = "이 논문의 핵심 질문은 \"딥러닝 기반 CSI 피드백 네트워크의 부동소수점 코드워드를 실제 통신 시스템의 유한 비트 피드백 링크에 맞게 양자화하면서도, 복원 성능 저하를 최소화할 수 있는가?\"이다.\n\n기존 CsiNet/CRNet은 연속 실수값 코드워드를 출력하지만, 실제 무선 채널은 유한 비트만 전송할 수 있어 양자화가 불가피하다. 본 연구는 양자화 인식 학습(QAT)을 도입하여 학습 단계에서 미리 양자화 효과를 시뮬레이션하고, STE로 미분 불가능한 양자화 함수를 우회하여 역전파를 가능하게 했다. 결과적으로 4비트 양자화만으로도 부동소수점에 근접하는 NMSE를 달성했다."
papers[8]['key_contributions'] = [
    "문제 정의: 기존 CSI 피드백 연구들이 부동소수점 코드워드를 가정하여, 유한 비트 피드백이라는 실제 통신 환경과의 간극이 존재함을 지적했다.",
    "핵심 기법: 양자화 인식 학습(QAT)을 CSI 피드백에 최초로 체계적으로 적용하여, 순방향에서는 양자화를 적용하고 역방향에서는 STE로 그래디언트를 전달하는 학습 파이프라인을 설계했다.",
    "설계 차별점: 균일 양자화뿐 아니라 코드워드 분포에 적응하는 학습 가능한 비균일 양자화 레벨을 제안하여, 코드워드 통계에 맞춤화된 양자화를 구현했다.",
    "실험 검증: CsiNet과 CRNet 코드워드에 4비트 QAT를 적용했을 때, 부동소수점(32비트) 수준에 근접하는 NMSE를 달성하여 8배의 피드백 비트 절감을 입증했다.",
    "실용성: 기존 CSI 피드백 네트워크에 플러그인 형태로 적용할 수 있어, 네트워크 구조 변경 없이 실용적 배포가 가능하다.",
    "연구사적 의미: CSI 피드백의 '학습-양자화 불일치' 문제를 최초로 체계적으로 다루어, 이후 혼합 정밀도, 적응적 비트 할당, 결합 최적화 연구의 출발점이 되었다."
]
papers[8]['key_equations'] = [
    {
        "name": "균일 양자화",
        "latex": "Q(x) = \\Delta \\cdot \\left\\lfloor \\frac{x}{\\Delta} + \\frac{1}{2} \\right\\rfloor, \\quad \\Delta = \\frac{x_{\\max} - x_{\\min}}{2^B - 1}",
        "description": "해석: 연속 실수 값을 일정 간격(Δ)으로 나눈 격자점에 반올림하는 연산이다. B비트이면 2^B개의 레벨로 나누며, 간격 Δ는 값 범위(x_max - x_min)를 레벨 수로 나눈 것이다. 예를 들어 4비트이면 16개 레벨, 8비트이면 256개 레벨을 사용한다. 비트 수가 적을수록 간격이 넓어져 양자화 오차가 커지지만, 피드백 비트 수는 줄어든다."
    },
    {
        "name": "직통 추정기 (STE)",
        "latex": "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} \\approx \\frac{\\partial \\mathcal{L}}{\\partial Q(\\mathbf{w})}",
        "description": "해석: 양자화 함수 Q(·)는 계단 함수이므로 미분이 거의 모든 곳에서 0이 되어 역전파가 불가능하다. STE는 이 문제를 우회하기 위해, 역전파 시 양자화 단계를 '없는 것처럼' 항등 함수로 대체한다. 즉 순방향에서는 양자화된 값을 사용하고, 역방향에서는 양자화 전의 연속값으로 그래디언트를 전달하여 네트워크가 양자화에 강건하게 학습된다."
    },
    {
        "name": "총 피드백 비트 수",
        "latex": "B_{\\text{total}} = M \\times B = \\frac{N_t \\times N_c}{\\gamma} \\times B",
        "description": "해석: 실제 전송해야 하는 총 비트 수는 코드워드 길이(M)와 원소당 비트 수(B)의 곱이다. M은 압축비 γ에 의해 결정되고, B는 양자화 비트에 의해 결정된다. 예를 들어 N_t×N_c=1024, γ=16이면 M=64이고, B=4비트이면 B_total=256비트만 전송하면 된다. 이 값이 곧 무선 채널을 차지하는 실제 피드백 오버헤드이다."
    }
]
papers[8]['architecture_detail'] = """2.1 문제 설정
딥러닝 기반 CSI 피드백 네트워크(CsiNet, CRNet 등)는 인코더가 연속 실수값 코드워드를 출력한다. 하지만 실제 무선 채널은 유한 비트만 전송할 수 있으므로, 코드워드를 이산화(양자화)해야 한다.

2.2 기존 접근의 한계
연속값 코드워드를 학습한 후 양자화를 '나중에 적용'하면, 네트워크는 양자화 왜곡에 대비하지 못해 성능이 크게 떨어진다. 특히 저비트(4비트 이하) 양자화에서 이 불일치가 심각하다.

2.3 양자화 인식 학습(QAT) 설계 철학
핵심 전략은 \"시험 전에 시험 환경과 똑같이 연습하자\"이다. 학습 과정에서 미리 양자화 효과를 시뮬레이션하여, 네트워크가 양자화 왜곡에 강건한 코드워드를 출력하도록 훈련한다.

2.4 핵심 기법 상세
- 순방향(Forward): 코드워드에 균일 양자화 Q(·)를 적용하여 실제 배포와 동일한 조건 생성
- 역방향(Backward): STE(Straight-Through Estimator)로 미분 불가능한 양자화를 우회하여 그래디언트 전달
- 학습 가능한 양자화: 양자화 레벨을 고정하지 않고 코드워드 분포에 맞게 학습하는 비균일 양자화
- 플러그인 적용: 기존 CsiNet/CRNet에 양자화 레이어만 삽입하여 구조 변경 없이 적용 가능

2.5 수식이 말하는 의미
- 균일 양자화식: 연속값을 이산 격자점으로 매핑 → 비트 표현의 수학적 정의
- STE식: 양자화의 미분 불가능성 우회 → QAT를 가능하게 하는 핵심 메커니즘
- 총 피드백 비트식: 압축비(γ)와 양자화 비트(B)의 곱으로 결정되는 실제 통신 오버헤드

2.6 이 논문이 남긴 것
코드워드 양자화 문제를 체계적으로 다룬 최초의 연구로, 이론과 실제 배포 사이의 간극을 좁혔다. 이후 혼합 정밀도 양자화, 적응적 비트 할당, BCsiNet(이진화), 결합 압축-양자화 등 다양한 양자화 연구의 기초가 되었다."""

# ============================================================
# Paper 10: BCsiNet - difficulty: advanced
# ============================================================
papers[10]['difficulty_level'] = 'advanced'
papers[10]['prerequisites'] = [
    "양자화 인식 학습(QAT)과 STE(Straight-Through Estimator)의 동작 원리",
    "이진 신경망(BNN)의 개념: 가중치와 활성화를 ±1로 양자화",
    "XNOR 연산과 비트카운트(popcount)로 내적을 근사하는 원리",
    "배치 정규화(Batch Normalization)의 역할과 BNN에서의 중요성"
]
papers[10]['learning_objectives'] = [
    "이진 신경망에서 sign 함수와 STE의 결합이 학습을 가능하게 하는 원리를 설명할 수 있다",
    "XNOR-비트카운트 연산이 일반 행렬곱을 대체하는 방법을 이해한다",
    "스케일링 팩터 α가 이진화 오차를 줄이는 역할을 설명할 수 있다",
    "BCsiNet의 극단적 경량화와 성능 손실 간 트레이드오프를 분석할 수 있다"
]
papers[10]['self_check_questions'] = [
    "sign 함수의 미분이 거의 모든 곳에서 0인데, BNN은 어떻게 학습이 가능한가?",
    "XNOR-비트카운트 연산이 일반 곱셈-덧셈보다 빠른 이유는 무엇인가?",
    "스케일링 팩터 α = ||w||₁/n 으로 설정하는 이유는 무엇인가?",
    "BCsiNet이 실용적일 수 있는 시나리오와 한계가 있는 시나리오는 각각 무엇인가?"
]
papers[10]['abstract'] = "이 논문의 핵심 질문은 \"CSI 피드백 네트워크의 가중치와 활성화를 모두 1비트(±1)로 극단적으로 양자화해도, 실용적으로 수용 가능한 수준의 CSI 복원이 가능한가?\"이다.\n\n기존 양자화 연구(QAT)가 4~8비트를 다루었다면, BCsiNet은 가장 극단적인 1비트 양자화를 탐구한다. 가중치와 활성화를 모두 sign 함수로 이진화하고, STE로 역전파를 우회하며, L1 노름 기반 스케일링 팩터로 크기 정보를 보존한다. XNOR + 비트카운트로 합성곱을 근사하여 메모리 32배 절감과 대폭적인 연산 속도 향상을 달성하면서도, 허용 가능한 NMSE를 유지한다."
papers[10]['key_contributions'] = [
    "문제 정의: IoT 기기나 초저전력 단말에서 기존 CSI 피드백 네트워크를 실행하기에는 메모리와 연산량이 지나치게 크다는 실용적 한계를 제시했다.",
    "핵심 기법: 가중치와 활성화를 모두 sign 함수로 1비트 이진화하고, L1 노름 기반 스케일링 팩터(α)로 크기 정보를 보존하는 이진 신경망(BNN)을 CSI 피드백에 최초로 적용했다.",
    "설계 차별점: XNOR과 비트카운트(popcount) 연산으로 합성곱을 근사하여 곱셈을 완전히 제거하고, 배치 정규화(BN)와 점진적 이진화 학습으로 극단적 양자화의 성능 저하를 완화했다.",
    "실험 검증: 32비트 CsiNet 대비 메모리를 약 32배 줄이고 연산 속도를 대폭 향상시키면서도, 높은 압축비에서 수용 가능한 NMSE를 달성했다.",
    "실용성: IoT, 초저전력 센서, FPGA 등 극도로 제한된 하드웨어 환경에서의 CSI 피드백 배포 가능성을 열었다.",
    "연구사적 의미: CSI 피드백 모델 압축의 극한을 탐구하여, 이진-삼진-다중비트 양자화의 스펙트럼에서 가장 극단적인 지점의 실현 가능성을 검증했다."
]
papers[10]['key_equations'] = [
    {
        "name": "가중치 이진화",
        "latex": "\\mathbf{w}_b = \\alpha \\cdot \\text{sign}(\\mathbf{w}), \\quad \\alpha = \\frac{\\|\\mathbf{w}\\|_{\\ell_1}}{n}",
        "description": "해석: 가중치를 sign 함수로 +1 또는 -1로 이진화한다. 이때 L1 노름의 평균값을 스케일링 팩터(α)로 곱하여 크기 정보를 보존한다. α가 없으면 모든 가중치가 동일한 절대값(1)을 가져 표현력이 극도로 제한되지만, α를 통해 원본 가중치의 평균적인 크기를 복원한다. 수학적으로 αb ≈ W가 되어, 이진 벡터와 스칼라의 곱으로 원본을 근사하는 것이다."
    },
    {
        "name": "XNOR-비트카운트 연산",
        "latex": "\\mathbf{w}_b^T \\mathbf{a}_b \\approx \\alpha \\beta \\cdot \\text{bitcount}(\\text{XNOR}(\\text{sign}(\\mathbf{w}), \\text{sign}(\\mathbf{a})))",
        "description": "해석: 이진화된 가중치와 활성화의 내적을 비트 연산만으로 계산하는 방법이다. ±1의 곱은 부호가 같으면 +1, 다르면 -1이므로 XNOR 논리 연산으로 대체된다. 결과의 합(내적)은 1의 개수를 세는 비트카운트(popcount)로 계산한다. 32비트 곱셈-덧셈 대신 1비트 XNOR-popcount를 사용하므로 이론적으로 수십 배 빠르고, 메모리도 32배 절감된다."
    }
]
papers[10]['architecture_detail'] = """2.1 문제 설정
IoT 기기나 초저전력 센서 같은 극도로 제한된 하드웨어 환경에서 CSI 피드백 네트워크를 실행하려면, 기존 32비트 부동소수점 연산은 메모리와 전력 소비 면에서 비현실적이다.

2.2 기존 접근의 한계
4~8비트 양자화(QAT)는 성능과 효율의 균형이 좋지만, 극한 환경에서는 여전히 부담스러울 수 있다. 1비트까지 줄일 수 있다면 메모리와 연산에서 획기적인 절감이 가능하다.

2.3 BCsiNet 설계 철학
\"곱셈을 완전히 없애자\"가 핵심이다. 가중치와 활성화를 모두 ±1로 이진화하면, 합성곱의 곱셈이 XNOR 논리 연산으로, 덧셈이 비트카운트(popcount)로 대체되어 하드웨어 효율이 극대화된다.

2.4 핵심 기법 상세
- Sign 이진화: sign(w)로 가중치를 +1/-1로 변환, sign(a)로 활성화도 이진화
- 스케일링 팩터 α: L1 노름 평균으로 계산하여 이진 가중치에 크기 정보 복원
- STE 역전파: sign 함수의 미분 불가능성을 항등 함수로 우회
- 배치 정규화(BN): 이진화 전에 활성화를 정규화하여 sign 함수의 입력 분포를 안정화
- 점진적 이진화: 초기에는 실수 가중치로 학습 후 점진적으로 이진화 비율을 높여 학습 안정성 확보

2.5 수식이 말하는 의미
- 이진화식: 가중치를 ±1 + 스케일링 팩터로 근사 → 32배 메모리 절감의 수학적 기반
- XNOR-비트카운트식: 곱셈을 논리 연산으로 대체 → 하드웨어 수준의 연산 가속

2.6 이 논문이 남긴 것
BCsiNet은 CSI 피드백 모델 압축의 극한을 탐구한 연구이다. 1비트 양자화의 실현 가능성을 보여줌으로써, 이후 삼진(TCsiNet, {-1,0,+1})이 '0 상태'를 추가하여 더 나은 정확도-효율 균형을 달성하는 방향으로 발전했다."""

# ============================================================
# Paper 13: CSI-GPT - difficulty: advanced
# ============================================================
papers[13]['difficulty_level'] = 'advanced'
papers[13]['prerequisites'] = [
    "GPT의 자기회귀(Autoregressive) 생성 원리와 Transformer 디코더 구조",
    "사전학습(Pre-training)과 미세조정(Fine-tuning)의 전이학습 패러다임",
    "연합학습(Federated Learning)의 기본 원리와 데이터 프라이버시 보호",
    "CSI 피드백에서 환경 일반화(Cross-Scenario Generalization)의 중요성"
]
papers[13]['learning_objectives'] = [
    "GPT 스타일 자기회귀 생성이 CSI 피드백에 적용되는 원리를 설명할 수 있다",
    "사전학습-미세조정 패러다임이 환경 일반화 문제를 해결하는 방식을 이해한다",
    "연합학습이 CSI 데이터 프라이버시를 보호하면서 모델을 개선하는 원리를 설명할 수 있다",
    "기존 CsiNet 계열과 CSI-GPT의 접근 방식 차이를 분석할 수 있다"
]
papers[13]['self_check_questions'] = [
    "CSI-GPT에서 CSI 원소를 순차적으로 예측하는 자기회귀 방식의 장점과 단점은 무엇인가?",
    "사전학습 없이 특정 환경에서만 학습한 모델이 새로운 환경에서 성능이 떨어지는 이유는?",
    "연합학습에서 글로벌 모델을 가중 평균으로 업데이트하는 이유는 무엇인가?",
    "CSI-GPT와 CsiNet의 가장 근본적인 설계 철학 차이는 무엇인가?"
]
papers[13]['abstract'] = "이 논문의 핵심 질문은 \"GPT 스타일의 대규모 생성 모델을 CSI 피드백에 적용하고, 연합학습으로 환경별 적응을 수행하면, 다양한 안테나 구성과 채널 환경에 걸쳐 일반화되는 범용 CSI 모델을 만들 수 있는가?\"이다.\n\n기존 CSI 피드백 모델은 특정 환경(실내/실외, 특정 안테나 수)에서 학습하면 다른 환경에서 성능이 급락하는 일반화 문제가 있었다. CSI-GPT는 다양한 환경의 CSI 데이터로 대규모 Transformer를 사전학습하여 범용적 CSI 표현을 학습하고, 각 기지국에서 원본 데이터를 공유하지 않고 연합학습으로 미세조정하여 환경별 적응과 프라이버시 보호를 동시에 달성한다."
papers[13]['key_contributions'] = [
    "문제 정의: 기존 CSI 피드백 모델이 특정 환경에 과적합(overfitting)되어 새로운 환경에서 성능이 급락하는 일반화 문제를 근본적으로 해결하고자 했다.",
    "핵심 기법: GPT 스타일의 자기회귀 Transformer를 CSI에 적용하여, CSI 행렬의 원소를 순차적으로 예측하는 생성적 사전학습을 수행했다.",
    "설계 차별점: 다양한 채널 환경과 안테나 구성의 대규모 CSI 데이터로 사전학습한 후, 각 기지국에서 연합학습(Federated Fine-Tuning)으로 로컬 적응을 수행하여 데이터 프라이버시를 보호했다.",
    "실험 검증: 다양한 안테나 구성(32×2, 64×4 등)과 채널 모델(실내/실외/도심)에 걸쳐, 환경별 개별 학습 대비 우수하거나 동등한 복원 성능을 달성했다.",
    "실용성: 하나의 사전학습 모델로 다양한 환경에 적응할 수 있어, 환경마다 별도 모델을 학습·배포하는 비용을 대폭 절감한다.",
    "연구사적 의미: CSI 피드백에 '파운데이션 모델(Foundation Model)' 패러다임을 최초로 도입하여, WiFo-CF 등 후속 대규모 모델 연구의 시발점이 되었다."
]
papers[13]['key_equations'] = [
    {
        "name": "자기회귀 CSI 예측",
        "latex": "p(\\mathbf{H}) = \\prod_{i=1}^{N} p(h_i | h_1, \\ldots, h_{i-1}; \\theta)",
        "description": "해석: CSI 행렬의 원소를 하나씩 순서대로 예측하는 자기회귀 생성 방식이다. 각 원소 h_i는 '지금까지 본 원소들(h_1,...,h_{i-1})'에 조건부로 생성되며, GPT가 다음 단어를 예측하는 것과 같은 원리이다. 이 방식은 CSI 원소 간의 순차적 의존성을 명시적으로 모델링하여, 전체 CSI 분포를 학습할 수 있다. 사전학습 단계에서 이 예측 과제를 대규모로 수행하여 범용적 CSI 표현을 습득한다."
    },
    {
        "name": "연합 집계",
        "latex": "\\theta_{\\text{global}} = \\sum_{k=1}^{K} \\frac{n_k}{n} \\theta_k",
        "description": "해석: K개 기지국의 로컬 모델 파라미터를 중앙 서버에서 가중 평균하여 글로벌 모델을 업데이트한다. 각 기지국 k의 데이터 양 n_k에 비례하여 가중하므로, 데이터가 많은 기지국의 학습 결과가 더 크게 반영된다. 중요한 점은 원본 CSI 데이터가 기지국 밖으로 나가지 않고 모델 파라미터만 공유되므로, 개별 사용자의 채널 정보가 프라이버시 보호된다는 것이다."
    }
]
papers[13]['architecture_detail'] = """2.1 문제 설정
실제 통신 환경은 실내, 실외, 도심, 교외 등 매우 다양하고, 안테나 구성(32×2, 64×4 등)도 기지국마다 다르다. 기존 CSI 피드백 모델은 특정 환경에서만 학습하므로, 새로운 환경에서 성능이 급락하는 일반화 문제가 심각하다.

2.2 기존 접근의 한계
CsiNet 계열은 특정 시나리오의 데이터로 학습하면 해당 환경에서는 뛰어나지만, 학습에 포함되지 않은 새로운 환경에서는 재학습이 필요하다. 환경마다 별도 모델을 학습·배포하는 것은 비용과 관리 측면에서 비현실적이다.

2.3 CSI-GPT 설계 철학
\"하나의 모델이 모든 환경을 커버하자\"가 핵심이다. NLP의 GPT처럼 대규모 데이터로 사전학습하여 범용적 표현을 학습한 후, 각 환경에는 가볍게 미세조정만 수행한다. 연합학습을 통해 데이터를 공유하지 않고도 환경별 적응이 가능하다.

2.4 핵심 기법 상세
- 자기회귀 생성: CSI 원소를 순차적으로 예측하는 GPT 스타일의 Transformer 디코더
- 사전학습: 다양한 환경의 대규모 CSI 데이터로 다음 원소 예측 과제를 수행하여 범용적 CSI 표현 학습
- 연합 미세조정: 각 기지국이 로컬 데이터로 모델을 미세조정하고, 서버에서 파라미터를 가중 평균으로 집계
- 프라이버시 보호: 원본 CSI 데이터는 기지국 밖으로 나가지 않고, 모델 파라미터만 교환

2.5 수식이 말하는 의미
- 자기회귀 예측식: CSI의 순차적 구조를 활용한 생성적 학습 → 범용적 CSI 분포 모델링
- 연합 집계식: 데이터 프라이버시를 유지하면서 분산 학습 → 실용적 환경 적응

2.6 이 논문이 남긴 것
CSI-GPT는 CSI 피드백에 파운데이션 모델 패러다임을 최초로 도입한 연구이다. '사전학습 → 미세조정'이라는 전이학습 접근법과 연합학습의 결합을 제안하여, 이후 WiFo-CF(MoE 파운데이션 모델), LVM4CF(비전 모델 활용) 등 대규모 모델 연구의 시발점이 되었다."""

# ============================================================
# Paper 33: VQ-VAE - difficulty: intermediate
# ============================================================
papers[33]['difficulty_level'] = 'intermediate'
papers[33]['prerequisites'] = [
    "VAE(Variational Autoencoder)의 기본 구조와 잠재 공간 개념",
    "벡터 양자화(Vector Quantization)와 코드북의 개념",
    "Straight-Through Estimator(STE)의 역할",
    "연속 잠재 표현 vs 이산 잠재 표현의 차이와 각각의 장점"
]
papers[33]['learning_objectives'] = [
    "VQ-VAE의 3가지 손실 항목(복원/코드북/커미트먼트)의 역할을 각각 설명할 수 있다",
    "stop-gradient(sg) 연산자가 코드북 학습과 인코더 학습을 분리하는 원리를 이해한다",
    "코드북 붕괴(codebook collapse) 문제의 원인과 해결 방법을 설명할 수 있다",
    "VQ-VAE의 이산 표현이 CSI 피드백 VQ 연구에 미친 영향을 분석할 수 있다"
]
papers[33]['self_check_questions'] = [
    "VQ-VAE에서 stop-gradient(sg) 연산자가 없으면 어떤 문제가 발생하는가?",
    "코드북 붕괴(codebook collapse)란 무엇이며, 왜 발생하는가?",
    "VQ-VAE의 이산 잠재 표현이 연속 VAE 대비 가지는 장점은 무엇인가?",
    "VQ-VAE의 커미트먼트 손실 계수 β를 너무 크거나 작게 설정하면 어떤 일이 일어나는가?"
]
papers[33]['abstract'] = "이 논문의 핵심 질문은 \"VAE의 연속 잠재 공간을 이산 코드북으로 대체하면, 사후 분포 붕괴(posterior collapse) 없이 고품질 생성을 달성하면서도 이산적 표현의 장점(코드 전송, 명확한 분류)을 얻을 수 있는가?\"이다.\n\nVQ-VAE는 인코더 출력 벡터를 학습된 코드북에서 가장 가까운 벡터로 교체(최근접 이웃 탐색)하여 이산 코드를 생성하고, Straight-Through Estimator로 양자화 단계의 그래디언트를 전파한다. 복원 손실, 코드북 손실, 커미트먼트 손실의 세 가지 손실 함수를 결합하여 인코더와 코드북을 동시에 학습하며, 이미지·비디오·음성에서 고품질 생성을 달성했다."
papers[33]['key_contributions'] = [
    "문제 정의: 기존 VAE의 연속 잠재 공간이 사후 분포 붕괴(posterior collapse) 문제를 겪으며, 이산적 표현의 자연스러운 장점을 활용하지 못하는 한계를 지적했다.",
    "핵심 기법: 인코더 출력을 학습 가능한 코드북의 최근접 벡터로 양자화하는 벡터 양자화(VQ) 계층을 VAE에 도입하여, 이산 잠재 표현을 학습하는 VQ-VAE를 제안했다.",
    "설계 차별점: stop-gradient(sg) 연산자를 활용하여 코드북 학습과 인코더 학습을 분리하고, 3가지 손실(복원/코드북/커미트먼트)로 각각의 학습 방향을 독립적으로 제어했다.",
    "실험 검증: 이미지(CIFAR-10, ImageNet), 비디오, 음성 생성 과제에서 연속 VAE를 능가하는 생성 품질을 달성하고, 사후 분포 붕괴 없이 안정적 학습을 보였다.",
    "실용성: 이산 코드(인덱스)만 전송하면 되므로 통신 비트 효율이 높고, 하위 생성 모델(PixelCNN 등)과 결합하여 고해상도 생성이 가능하다.",
    "연구사적 의미: 이산 표현 학습의 기초를 세워, CSI 피드백의 VQ 기반 양자화, SoundStream(RVQ), FSQ 등 다양한 분야로 확산된 핵심 참고 논문이 되었다."
]
papers[33]['key_equations'] = [
    {
        "name": "VQ-VAE 손실 함수",
        "latex": "\\mathcal{L} = \\|\\mathbf{x} - D(\\mathbf{e}_k)\\|_2^2 + \\|\\text{sg}[E(\\mathbf{x})] - \\mathbf{e}_k\\|_2^2 + \\beta\\|E(\\mathbf{x}) - \\text{sg}[\\mathbf{e}_k]\\|_2^2",
        "description": "해석: 세 개의 손실 항목이 서로 다른 학습 목표를 담당한다. 첫째 항(복원 손실)은 디코더가 입력을 잘 재현하도록 한다. 둘째 항(코드북 손실)은 sg(stop-gradient)로 인코더를 고정한 채 코드북 벡터 e_k를 인코더 출력 쪽으로 이동시킨다. 셋째 항(커미트먼트 손실)은 반대로 코드북을 고정한 채 인코더 출력이 코드북 벡터에 가까워지도록 한다. β 계수로 인코더의 커미트먼트 강도를 조절하며, 이 세 항의 균형이 안정적 학습의 핵심이다."
    },
    {
        "name": "벡터 양자화",
        "latex": "\\mathbf{z}_q = \\mathbf{e}_k, \\quad k = \\arg\\min_j \\|E(\\mathbf{x}) - \\mathbf{e}_j\\|_2",
        "description": "해석: 인코더 출력 벡터 z_e = E(x)를 코드북의 모든 벡터 {e_1, e_2, ..., e_K}와 L2 거리를 비교하여, 가장 가까운 벡터 e_k로 교체한다. 이것이 '양자화'의 핵심 연산으로, 연속 벡터를 K개의 이산 코드 중 하나로 매핑한다. 전송 시에는 인덱스 k만 보내면 되므로 log₂(K) 비트만 필요하다. 이 아이디어가 CSI 피드백 VQ 연구의 직접적 기반이 되었다."
    }
]
papers[33]['architecture_detail'] = """2.1 문제 설정
기존 VAE는 연속 잠재 공간을 사용하지만, 많은 실제 데이터(음성, 텍스트, 코드 등)는 본질적으로 이산적 구조를 가진다. 연속 잠재 변수를 사용하면 사후 분포 붕괴(posterior collapse) 문제가 발생할 수 있다.

2.2 기존 접근의 한계
연속 VAE는 KL 발산 정규화로 인해 잠재 코드가 사전 분포에 과도하게 가까워지면서 디코더가 잠재 코드를 무시하는 현상(posterior collapse)이 발생한다. 또한 연속 표현은 직접 전송에 부적합하여 별도의 양자화가 필요하다.

2.3 VQ-VAE 설계 철학
\"잠재 공간 자체를 이산으로 만들자\"가 핵심이다. 인코더 출력을 학습 가능한 코드북의 최근접 벡터로 교체하여 이산 코드를 생성하고, STE로 역전파를 가능하게 한다.

2.4 핵심 기법 상세
- 인코더: 입력을 연속 벡터 z_e로 변환
- 벡터 양자화: z_e를 코드북 {e_1,...,e_K}에서 최근접 벡터 e_k로 교체
- 디코더: 양자화된 e_k로부터 입력을 복원
- STE: 순방향은 양자화, 역방향은 그래디언트를 디코더에서 인코더로 직접 전달
- 3가지 손실: 복원(전체 파이프라인), 코드북(sg[E(x)] → e_k), 커미트먼트(E(x) → sg[e_k])
- EMA 갱신: 코드북 벡터를 지수 이동 평균으로 갱신하여 학습 안정성 확보

2.5 수식이 말하는 의미
- VQ-VAE 손실식: sg로 분리된 3방향 동시 학습 → 인코더·코드북·디코더 각각의 역할 명확화
- 벡터 양자화식: 연속 → 이산 매핑의 수학적 정의 → log₂(K) 비트 전송의 기반

2.6 이 논문이 남긴 것
VQ-VAE는 이산 표현 학습의 기초를 세운 논문이다. CSI 피드백의 VQ 기반 코드워드 양자화, SoundStream의 RVQ, FSQ의 스칼라 양자화 등 다양한 분야에서 이 아이디어가 활용되고 있으며, 양자화 기반 표현 학습의 참조점이 되었다."""

# Write back the complete file
with open(r'C:\Users\hyunj\csiautoencoder\public\data\initial-papers.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

print("Successfully enhanced 11 papers!")
print(f"Total papers: {len(papers)}")
print(f"Total relationships: {len(data['relationships'])}")

# Verify all enhanced papers have the new fields
enhanced_indices = [0, 1, 2, 3, 4, 5, 6, 8, 10, 13, 33]
for idx in enhanced_indices:
    p = papers[idx]
    title = p['title'][:50]
    has_diff = 'difficulty_level' in p
    has_prereq = 'prerequisites' in p
    has_learn = 'learning_objectives' in p
    has_check = 'self_check_questions' in p
    print(f"  Paper {idx} ({title}...): diff={has_diff}, prereq={has_prereq}, learn={has_learn}, check={has_check}")
