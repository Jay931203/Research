import fs from 'fs';

const data = JSON.parse(fs.readFileSync('public/data/initial-papers.json', 'utf-8'));

// ============================================================
// Learner-friendly architecture_detail rewrites (Korean)
// Titles stay in English. Everything else is Korean.
// Style: 쉬운 비유 → 핵심 구조 → 왜 중요한지
// ============================================================

const archDetails = {
  0: `쉽게 말해 \"이미지 압축기\"처럼 동작하는 최초의 CSI 피드백 딥러닝 모델이다. 인코더(UE 측)는 CSI 행렬을 2D 이미지처럼 취급하여 3×3 합성곱(Conv2D)으로 특징을 추출한 뒤, 완전연결(FC) 레이어로 저차원 코드워드 벡터로 압축한다. 디코더(BS 측)는 FC 레이어로 대략적인 CSI를 복원한 후, RefineNet 블록 2개로 디테일을 보정한다. RefineNet은 Conv2D → BN → ReLU → Conv2D 구조에 skip connection(잔차 연결)을 더한 블록으로, 복원 정밀도를 한 단계씩 끌어올린다. 이 구조는 이후 대부분의 CSI 피드백 연구가 기본 뼈대로 삼는 \"원조 아키텍처\"이다.`,

  1: `CsiNet이 한 프레임(정지 사진)만 보는 모델이라면, CsiNet-LSTM은 여러 프레임(동영상)을 보는 모델이다. 기존 CsiNet의 CNN 레이어로 공간 특징을 추출한 뒤, LSTM 순환 레이어를 추가하여 \"이전 프레임에서 이미 보낸 정보\"를 기억한다. 덕분에 프레임 간 중복되는 부분을 다시 보내지 않아도 되어, 같은 품질을 유지하면서 피드백 오버헤드를 크게 줄일 수 있다. 시변 채널(사용자가 이동하는 상황)에서 특히 효과적이다.`,

  2: `\"여러 크기의 돋보기로 동시에 보자\"는 아이디어의 모델이다. 핵심 블록인 CRBlock은 3×3, 5×5, 7×7 등 서로 다른 크기의 합성곱 필터를 병렬로 적용한다. 작은 필터는 세부 패턴을, 큰 필터는 넓은 범위의 패턴을 포착하여, 이를 Squeeze-and-Excitation(SE) 채널 주의 메커니즘으로 \"어떤 필터의 결과가 더 중요한지\" 자동으로 가중치를 매겨 합친다. 인코더는 Conv2D + FC, 디코더는 FC + 여러 CRBlock 순으로 구성된다.`,

  3: `모바일 기기는 계산 자원이 제한적이라는 점에 착안한 경량 모델이다. 핵심 전략은 표준 합성곱을 \"깊이별 분리 합성곱(Depthwise Separable Conv)\"으로 교체하는 것이다. 일반 합성곱이 모든 채널을 한꺼번에 처리한다면, 분리 합성곱은 채널별로 따로 처리(DW)한 뒤 결과를 1×1 합성곱(PW)으로 섞는 2단계 방식이라 파라미터가 훨씬 적다. 추가로 보조 정제 네트워크(AnciNet)로 복원 품질을 보완하여, CRNet 수준의 성능을 1/K 수준의 파라미터만으로 달성한다.`,

  4: `CNN 대신 Transformer의 셀프 어텐션을 전면 도입한 최초의 CSI 피드백 모델이다. CNN은 작은 윈도우(예: 3×3) 내에서만 패턴을 보지만, 셀프 어텐션은 CSI 행렬의 모든 위치 간 관계를 한 번에 계산한다. CSI를 패치(작은 조각)로 나누고 위치 인코딩을 더한 뒤, Multi-Head Self-Attention 레이어로 전역 의존성을 학습한다. 인코더 어텐션 출력을 FC로 압축하고, 디코더에서 다시 어텐션으로 복원한다. 장거리 공간 의존성을 포착하는 데 유리하다.`,

  5: `\"공간과 주파수, 두 가지 시각으로 동시에 보자\"는 교차 도메인 접근법이다. 핵심인 AggregationBlock은 3×3·5×5·7×7 커널을 연결(concat)하고 1×1 Conv로 통합하며, 잔차 연결을 갖는다. 여기에 교차 도메인 주의 모듈이 추가되어, 공간 특징과 주파수 특징 사이의 상호작용을 학습하고 하다마드 곱(원소별 곱)으로 특징을 재조정한다. 결과적으로 CRNet보다 적은 파라미터로 더 좋은 NMSE를 달성한다.`,

  6: `\"멀리 떨어진 위치 간 관계도 살펴보자\"는 비국소(Non-Local) 주의와 경량 합성곱을 결합한 모델이다. 비국소 블록은 CSI 행렬 내 모든 위치 쌍의 유사도를 계산하여, 먼 곳에 있지만 관련된 채널 정보를 활용한다. 여기에 깊이별 분리 합성곱(DS Conv)을 사용해 파라미터를 줄였다. 즉, \"넓은 시야\"는 Non-Local 블록이, \"가벼운 계산\"은 DS Conv가 각각 담당하는 분업 구조이다.`,

  7: `기지국(BS)은 강력하지만 UE(사용자 단말)는 약한 계산 자원을 가진다는 현실에 맞춘 비대칭 설계이다. UE 측 인코더는 최소한의 합성곱만 사용해 계산 부담을 줄이고, BS 측 디코더는 깊은 네트워크로 복원 품질을 최대화한다. 비유하자면, 스마트폰(UE)은 가볍게 데이터를 요약만 하고, 서버(BS)가 무거운 복원 작업을 전담하는 구조이다.`,

  8: `학습 과정에서 양자화 효과를 미리 시뮬레이션하여, 실제 양자화 시에도 정확도가 크게 떨어지지 않게 하는 기법이다. 순방향(forward) 때는 균일 양자화를 적용하고, 역방향(backward) 때는 Straight-Through Estimator(STE)로 미분 불가능한 양자화 단계를 우회하여 그래디언트를 전달한다. 쉽게 말해, \"시험 전에 시험 환경과 똑같이 연습해 두면 실전에서도 잘한다\"는 전략이다. 기존 CSI 피드백 네트워크에 플러그인 형태로 적용할 수 있다.`,

  9: `시간에 따라 변하는 CSI를 효율적으로 피드백하기 위해, \"변한 부분만 보내자\"는 차분 인코딩 전략을 사용한다. 현재 CSI에서 이전 복원 CSI를 빼서 변화량(ΔH)만 인코딩하여 전송한다. CSI가 천천히 변하는 경우 ΔH는 매우 작아져 적은 비트로도 정확한 피드백이 가능하다. 마르코프 체인 관점에서 CSI의 시간 상관관계를 활용하는 구조이다.`,

  10: `가중치와 활성화를 모두 +1 또는 −1로 극단적으로 양자화하는 이진 신경망(BNN)이다. 일반 32비트 실수 연산 대신 XNOR과 비트카운트(popcount)로 합성곱을 근사하므로, 메모리 사용량이 약 32배 줄고 연산 속도도 대폭 향상된다. 정확도 손실을 줄이기 위해 배치정규화(BN)와 스케일링 팩터를 함께 학습한다. IoT처럼 초저전력 환경에서의 CSI 피드백에 적합하다.`,

  11: `큰 네트워크(교사)의 지식을 작은 네트워크(학생)에 전달하는 지식 증류 기법이다. 교사 네트워크는 고성능 CsiNet/CRNet이고, 학생 네트워크는 파라미터가 적은 경량 버전이다. 학생은 정답 라벨뿐 아니라 교사의 중간 특징맵과 소프트 출력도 모방하여 학습하므로, 작은 크기에 비해 높은 복원 성능을 보인다. 비유하면 \"경험 많은 선배가 핵심 노하우를 후배에게 전수\"하는 것과 같다.`,

  12: `공간 주의(Spatial Attention)와 채널 주의(Channel Attention)를 동시에 활용하는 듀얼 어텐션 구조이다. 공간 주의는 \"CSI 행렬에서 어느 위치가 중요한가\"를 학습하여 해당 영역을 강조하고, 채널 주의는 \"어떤 특징 채널이 유용한가\"를 판단하여 채널별 가중치를 조절한다. 두 주의 메커니즘을 결합하면, 네트워크가 핵심 정보에 집중하여 효율적으로 CSI를 압축·복원할 수 있다.`,

  13: `GPT 스타일의 자기회귀 트랜스포머를 CSI 피드백에 도입한 대규모 모델이다. CSI 행렬의 각 원소를 \"이전까지의 원소들\"에 조건부로 순차 예측한다. 다양한 채널 환경에서 수집한 대규모 CSI 데이터로 사전학습(pre-training)하고, 각 특정 환경에는 연합학습(Federated Tuning)으로 적응한다. 데이터 프라이버시를 유지하면서도 광범위한 환경에 적용할 수 있는 것이 강점이다.`,

  14: `압축비를 상황에 맞게 유연하게 바꿀 수 있는 가변 길이 코드워드 구조이다. 기존 CsiNet은 압축비마다 별도 모델을 학습해야 했지만, CsiNet+는 하나의 인코더에서 다양한 길이의 코드워드를 점진적으로 출력한다. 채널 상태가 좋을 때는 짧은 코드워드를, 나쁠 때는 긴 코드워드를 사용하여, 한 모델로 여러 압축비를 커버한다. 실용적인 적응형 FDD 시스템에 적합하다.`,

  15: `네트워크의 모든 레이어를 같은 비트로 양자화하는 대신, 각 레이어의 민감도에 따라 서로 다른 비트폭을 할당하는 혼합 정밀도 전략이다. 출력 변화에 민감한 레이어(주로 앞쪽과 뒤쪽)에는 8비트 등 높은 정밀도를 유지하고, 덜 민감한 중간 레이어에는 4비트나 2비트를 사용한다. 비유하자면, \"중요한 부분에는 고화질을, 덜 중요한 부분에는 저화질을 배정\"하여 전체 모델 크기를 줄이는 방식이다.`,

  16: `\"쓸모없는 연결은 잘라내자\"는 가지치기(Pruning) 기반 경량화이다. 각 합성곱 필터의 중요도를 L1-norm으로 측정하여, 점수가 낮은(영향이 적은) 필터를 통째로 제거한다. 제거 후 남은 네트워크를 미세조정(fine-tuning)하여 성능을 회복한다. 구조적 가지치기이므로 실제 추론 속도가 빨라진다는 장점이 있으며, 양자화와 조합하면 더 큰 경량화 효과를 얻을 수 있다.`,

  17: `코드워드의 모든 원소를 같은 비트로 양자화하는 대신, 원소별로 비트 수를 다르게 배정하는 적응적 할당 전략이다. 레이트-왜곡 최적화(R-D Optimization) 관점에서, 주어진 총 비트 예산 내에서 복원 오차를 최소화하는 최적 비트 분배를 학습한다. 쉽게 말해, \"정보가 많이 담긴 원소에는 비트를 많이, 별로 중요하지 않은 원소에는 비트를 적게\" 주는 전략이다.`,

  18: `모바일넷 계열의 ShuffleNet 기법을 CSI 피드백에 접목한 경량 모델이다. 그룹 합성곱(Group Conv)으로 채널을 그룹별로 나눠 처리하여 파라미터를 줄이고, 채널 셔플(Channel Shuffle)로 그룹 간 정보를 교환한다. 그룹 합성곱만 쓰면 그룹 간 정보 교류가 끊기는 문제가 있는데, 채널 셔플이 이를 해결해 준다. CsiNet 대비 훨씬 적은 파라미터로 유사한 복원 성능을 달성한다.`,

  19: `코드워드를 연속 실수가 아닌 \"코드북 벡터\"로 양자화하는 VQ-VAE 기반 구조이다. 인코더가 CSI를 연속 잠재 벡터로 인코딩하면, 학습된 코드북에서 가장 가까운 벡터를 찾아 대체한다. 디코더는 이 코드북 인덱스로부터 CSI를 복원한다. 장점은 이산 인덱스만 전송하면 되어 비트 효율이 높다는 것이며, 코드북 붕괴(일부 벡터만 사용되는 문제)를 방지하는 기법도 함께 적용된다.`,

  20: `최근 이미지 생성에서 큰 성과를 보인 확산 모델(Diffusion Model)을 CSI 복원에 도입한 구조이다. 순방향에서는 CSI에 점점 노이즈를 추가하고, 역방향에서는 수신된 코드워드를 조건으로 노이즈를 단계적으로 제거하여 CSI를 복원한다. 확산 모델의 강력한 생성 능력 덕분에 낮은 압축비에서도 세밀한 CSI 구조를 복원할 수 있다. 다만 추론 시 여러 단계를 반복해야 하므로 속도 면에서는 불리할 수 있다.`,

  21: `실제 통신 시스템에서는 압축과 양자화가 별도로 이뤄지면 성능이 떨어지는 문제가 있다. 이 연구는 인코더-양자화기-디코더를 하나의 파이프라인으로 묶어 엔드투엔드로 동시 최적화한다. 압축 손실과 양자화 손실을 하나의 결합 손실 함수로 학습하여, 양자화 왜곡에 강건한 압축 표현을 학습할 수 있다. \"이론과 현실의 간극을 좁히는\" 실용적인 접근법이다.`,

  22: `가중치를 {−1, 0, +1} 세 값만으로 표현하는 삼진 신경망이다. 이진 신경망(BNN)의 {−1, +1}에 0을 추가한 것으로, 0은 \"이 연결은 무시해도 된다\"는 의미를 갖기 때문에 암묵적인 가지치기 효과가 있다. 곱셈이 단순한 부호 반전과 0 처리로 대체되어 하드웨어 효율이 매우 높다. BNN보다 표현력이 높아 정확도 손실이 더 적으면서도 경량성을 유지한다.`,

  23: `오토인코더의 합성곱 블록에 \"지역 인식 어텐션(Local-Aware Attention)\"을 주입한 모델이다. 일반 합성곱은 모든 위치를 동일하게 처리하지만, 어텐션 모듈이 공간적으로 어느 영역이 중요한지 판단하여 해당 영역의 특징을 증폭시킨다. 인코더와 디코더 양쪽에 어텐션이 적용되어, 압축 단계에서는 핵심 정보를 선별하고 복원 단계에서는 세부 정보를 강화한다.`,

  24: `Swin Transformer의 \"이동 창 어텐션\"과 LSTM의 \"시간 기억력\"을 결합한 SwinLSTM 셀이 핵심이다. 공간 및 주파수 축에서는 Swin Transformer가 지역-전역 의존성을 포착하고, 시간 축에서는 LSTM이 프레임 간 변화를 추적한다. 시간·공간·주파수 3차원 CSI 데이터를 하나의 통합 아키텍처로 처리할 수 있어, 시변 환경에서 매우 효과적이다.`,

  25: `CSI 피드백 네트워크에 최적화된 양자화 설계를 다루는 연구이다. 핵심은 NMSE와 양자화 왜곡을 동시에 줄이는 적응적 결합 손실 함수와, 코드워드 원소별로 비트를 다르게 할당하는 비균일 비트 할당 전략이다. 양자화를 \"나중에 덧붙이는 후처리\"가 아니라 \"학습 과정의 일부\"로 통합하여, 양자화 후에도 복원 품질이 크게 떨어지지 않도록 한다.`,

  26: `가역 신경망(Invertible Neural Network)을 CSI 피드백에 적용한 독창적 모델이다. 일반 오토인코더는 인코더와 디코더가 별도 네트워크인데, 가역 네트워크는 같은 네트워크를 순방향으로 돌리면 인코딩, 역방향으로 돌리면 디코딩이 된다. 정보 손실 없는 가역 변환으로 CSI를 잠재 공간에 매핑하고, 내장(endogenous) 양자화기로 이산화하여 전송한다. 정보 보존 관점에서 이론적으로 우수하다.`,

  27: `의미 통신(Semantic Communication) 개념을 CSI 피드백에 도입한 모델이다. 기존 방식이 CSI 행렬의 모든 원소를 동등하게 압축하는 반면, SemCSINet은 \"통신 성능에 실제로 중요한 의미적 특징\"만 선별적으로 추출하여 전송한다. 수신 측에서는 의미적 특징으로부터 전체 CSI를 복원한다. 불필요한 정보는 과감히 버리는 전략으로 더 높은 압축 효율을 달성한다.`,

  28: `오토인코더의 초기 복원에 확산 모델을 \"보조 정제기\"로 붙인 2단계 구조이다. 1단계: 경량 오토인코더가 빠르게 CSI를 압축·초기 복원한다. 2단계: 확산 모델이 초기 복원의 잔차(오차 부분)를 추가로 보정하여 세밀한 디테일을 살린다. 확산 단계 수를 조절하여 압축률과 복원 품질의 균형을 유연하게 맞출 수 있으며, 가변 레이트(variable-rate) 전송이 가능하다.`,

  29: `대규모 CSI 데이터로 사전학습한 무선 통신 도메인 파운데이션 모델이다. 비전 분야의 대규모 모델처럼, 먼저 광범위한 환경의 CSI로 기반 모델을 훈련시킨 뒤, 특정 시나리오에는 S-R MoE(Sparse-to-Rich Mixture of Experts) 모듈로 효율적 미세조정(fine-tuning)한다. \"하나의 모델로 다양한 환경에 적응\"하는 것이 목표이며, 환경별로 별도 모델을 학습할 필요가 없어 실용성이 높다.`,

  30: `새로운 환경(학습 데이터에 없던 환경)에서도 잘 동작하는 일반화 능력이 핵심인 모델이다. CsiNet 기반 오토인코더에 SVD 기반 클러스터링과 분포 정렬 모듈을 추가했다. Eckart-Young-Mirsky 정리를 활용하여 서로 다른 환경의 CSI 분포를 정렬함으로써, 학습 환경과 테스트 환경이 달라도 성능 저하를 최소화한다. \"낯선 장소에 가도 적응할 수 있는\" 일반화를 추구한다.`,

  31: `GPT나 CLIP 같은 대규모 비전 모델(LVM)의 사전학습된 표현력을 CSI 피드백에 빌려 오는 접근법이다. CSI 행렬을 이미지처럼 취급하여 비전 모델의 인코더에 넣고, 오프라인으로 추출된 풍부한 특징 표현을 CSI 복원에 활용한다. 대규모 모델의 방대한 표현력 덕분에 적은 학습 데이터로도 좋은 복원 성능을 보이며, 비전 분야의 발전을 통신에 접목하는 교차 분야 연구이다.`,

  32: `대규모 언어 모델(LLM)을 채널 추정의 파일럿 설계에 활용하는 혁신적 시도이다. LLM이 채널 환경에 대한 텍스트 설명(예: \"도심 환경, 높은 빌딩 밀집\")을 입력받아 해당 환경에 최적화된 파일럿 패턴을 생성한다. 의미적 이해력을 가진 LLM이 물리적 환경의 특성을 반영한 파일럿을 설계하여, 전통적인 수학적 최적화 방식과 차별화된다.`,

  33: `이미지 생성의 VAE에서 연속적인 잠재 공간 대신 이산 코드북을 사용하는 생성 모델이다. 인코더 출력 벡터를 코드북에서 가장 가까운 벡터로 교체(nearest-neighbor lookup)하고, 이 인덱스를 디코더에 전달한다. 역전파 시에는 Straight-Through Estimator로 양자화를 우회한다. CSI 피드백 VQ 연구의 이론적 토대가 되는 핵심 참고 논문이다.`,

  34: `LLM의 가중치를 활성화 분포를 고려하여 양자화하는 사후 학습 기법이다. 핵심 아이디어는 \"활성화 값이 큰 채널에 연결된 가중치가 더 중요하다\"는 관찰이다. 활성화 크기에 비례하는 스케일링을 가중치에 적용하여, 중요한 가중치의 양자화 오차를 줄이고 덜 중요한 가중치는 과감히 양자화한다. 추가 학습 없이 적용 가능하여, 대규모 모델의 빠른 경량화에 적합하다.`,

  35: `각 레이어에 최적 비트폭을 자동으로 결정하는 혼합 정밀도 양자화 프레임워크이다. 핵심은 헤시안(Hessian) 행렬의 고유값으로 레이어 민감도를 측정하는 것이다. 고유값이 큰 레이어(출력 변화에 민감한 레이어)에는 높은 비트폭을, 작은 레이어에는 낮은 비트폭을 할당한다. \"2의 거듭제곱\" 비트폭만 사용하여(dyadic) 하드웨어 친화적이며, 수동 튜닝 없이 자동으로 최적 배분을 찾는다.`,

  36: `대규모 트랜스포머의 사후 학습 양자화를 위한 2차(second-order) 근사 방법이다. OBQ(Optimal Brain Quantization)의 열 단위 갱신을 행 단위로 전환하여 대규모 모델에도 적용 가능하게 만들었다. 한 가중치를 양자화할 때, 나머지 가중치를 헤시안 정보로 보정하여 전체 출력 변화를 최소화한다. 수십~수백억 파라미터 모델도 단일 GPU에서 몇 시간 만에 양자화할 수 있는 확장성이 특징이다.`,

  37: `VQ-VAE의 복잡한 코드북 학습을 완전히 제거하고, 각 잠재 차원을 유한 개의 정수 레벨로 단순 반올림하는 방식이다. 예를 들어 5개 차원을 각각 8레벨로 양자화하면 8^5 = 32,768개의 코드를 별도 코드북 없이 자동으로 얻는다. commitment loss나 EMA 갱신 같은 VQ 학습 트릭이 불필요하여 구현이 매우 간단하면서도, VQ-VAE와 유사한 성능을 보인다.`,

  38: `오디오 신호를 위한 엔드투엔드 신경 코덱으로, 잔차 벡터 양자화(RVQ)가 핵심이다. 인코더가 오디오를 잠재 벡터로 변환한 후, 여러 단계의 VQ를 순차 적용한다. 첫 단계에서 대략적으로 양자화하고, 이후 단계들이 잔차(나머지 오차)를 반복적으로 양자화하여 점점 정교해진다. RVQ 단계 수로 비트레이트를 유연하게 조절할 수 있으며, 이 RVQ 아이디어는 CSI 피드백의 벡터 양자화 연구에도 영감을 준다.`,

  39: `코드워드를 방향(형상)과 크기(이득)로 분해하여 각각 따로 양자화하는 VQ 기반 CSI 피드백이다. 단위 노름 방향 벡터(형상)는 구면 위의 코드북으로, 스칼라 크기(이득)는 별도의 스칼라 양자화기로 처리한다. 이렇게 분리하면 차원이 높아져도 코드북 크기가 기하급수적으로 폭발하는 문제를 완화할 수 있다. 방향과 크기의 서로 다른 통계적 특성을 각각 최적으로 양자화한다.`
};

// Apply architecture_detail updates
for (const [idx, detail] of Object.entries(archDetails)) {
  data.papers[parseInt(idx)].architecture_detail = detail;
}

// ============================================================
// Learner-friendly equation descriptions (Korean)
// ============================================================

const eqUpdates = {
  0: [
    { name: "CSI 압축", desc: "인코더가 CSI 행렬 H_a(채널 정보가 담긴 2D 행렬)를 압축비 γ에 따라 짧은 코드워드 벡터 s로 줄인다. M은 코드워드의 길이로, 안테나 수(N_t)와 부반송파 수(N_c)를 γ로 나눈 값이다." },
    { name: "NMSE 손실 함수", desc: "모델 학습의 목표 지표로, 원본 CSI와 복원된 CSI가 얼마나 다른지를 0~1 사이로 나타낸다. 값이 작을수록 복원이 잘 된 것이며, 분모로 정규화하여 CSI 크기에 무관하게 비교할 수 있다." },
    { name: "압축비", desc: "원본 CSI 크기(N_t × N_c) 대비 코드워드 크기(M)의 비율이다. γ=4이면 원본의 1/4 크기로 압축한다는 뜻이며, 값이 클수록 더 많이 압축하므로 복원이 어려워진다." }
  ],
  1: [
    { name: "시간적 CSI 피드백", desc: "인코더가 현재 CSI뿐 아니라 이전 시점의 은닉 상태(h_{t-1}, 이전 기억)도 함께 활용한다. 덕분에 이전 프레임과 겹치는 정보는 다시 보내지 않아도 된다." },
    { name: "LSTM 은닉 상태 갱신", desc: "LSTM이 현재 코드워드(s_t)와 이전 기억(h_{t-1})을 결합하여 새로운 기억(h_t)을 만드는 과정이다. 시간에 따라 변하는 CSI의 패턴을 누적적으로 학습한다." }
  ],
  2: [
    { name: "다중 해상도 특징 집계", desc: "서로 다른 크기의 필터(k₁, k₂, ...)가 각각 추출한 특징을 주의 가중치(α)로 합친다. 작은 필터는 세부 패턴을, 큰 필터는 넓은 패턴을 포착하며, 네트워크가 자동으로 최적 비중을 결정한다." },
    { name: "채널 주의 (SE 블록)", desc: "Squeeze-and-Excitation 방식으로, 먼저 전역 평균 풀링(GAP)으로 각 채널의 \"중요도 요약\"을 만든 뒤, FC 레이어와 시그모이드(σ)를 거쳐 0~1 사이의 채널별 가중치를 출력한다." }
  ],
  3: [
    { name: "깊이별 분리 합성곱", desc: "일반 합성곱을 2단계로 분해한 경량 연산이다. 1단계(DW): 각 채널을 독립적으로 k×k 합성곱. 2단계(PW): 1×1 합성곱으로 채널 간 정보를 혼합. 파라미터가 약 1/k² 수준으로 줄어든다." },
    { name: "CLNet 복잡도 감소", desc: "CLNet이 CRNet 대비 약 1/K의 파라미터만 사용함을 보여준다. K는 커널 크기 관련 상수로, 분리 합성곱의 경량화 효과를 정량적으로 나타낸다." }
  ],
  4: [
    { name: "다중 헤드 셀프 어텐션", desc: "CSI 패치들 사이의 관계를 계산하는 핵심 연산이다. 각 패치에서 Query(질문), Key(열쇠), Value(값)를 만들고, Q와 K의 내적으로 \"얼마나 관련 있는지\" 점수를 매긴 뒤, 그 점수로 V를 가중합한다." },
    { name: "다중 헤드 출력", desc: "서로 다른 관점으로 어텐션을 h번 수행한 결과를 이어붙인(Concat) 후, 출력 가중치 W^O로 최종 변환한다. 하나의 어텐션보다 다양한 패턴을 동시에 포착할 수 있다." }
  ],
  5: [
    { name: "교차 도메인 특징 융합", desc: "공간 도메인 특징(F_spatial)과 주파수 도메인 특징(F_freq)을 결합하는 연산이다. 교차 주의(A_cross)로 만든 가중치를 공간 특징에 원소별로 곱한 뒤, 주파수 특징을 더한다." },
    { name: "집계 블록", desc: "3×3, 5×5, 7×7 세 가지 크기의 합성곱 결과를 이어붙이고(Cat) 하나의 합성곱으로 통합한 뒤, 입력과 더하는(잔차 연결) 구조이다. 다양한 스케일의 패턴을 동시에 활용한다." }
  ]
};

for (const [paperIdx, eqs] of Object.entries(eqUpdates)) {
  const paper = data.papers[parseInt(paperIdx)];
  if (paper.key_equations) {
    for (const update of eqs) {
      const eq = paper.key_equations.find(e => e.name === update.name);
      if (eq) eq.description = update.desc;
    }
  }
}

fs.writeFileSync('public/data/initial-papers.json', JSON.stringify(data, null, 2) + '\n', 'utf-8');
console.log('Updated all architecture_detail descriptions (40 papers) and equation descriptions (6 papers core equations) to be learner-friendly.');
